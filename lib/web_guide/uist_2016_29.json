{"$id":"database","$priority":null,"annotation":{"headers":["_id","name","type","description","icon","icon_url","sequence"]},"conference":{"headers":["id","name","location","utc_offset","description","webapp_base_url","icon_url","start_day","num_days","data_sync","vote","reading_list","schedule","note","schema_version"],"rows":[["uist_2016","UIST 2016","Tokyo, JP",540,"29th ACM Symposium on User Interface Software and Technology","http://confapp.github.io/web_guide/?conference=uist_2016","https://firebasestorage.googleapis.com/v0/b/confapp-data-sync.appspot.com/o/uist_2016%2Ficons%2FLogo-simple.png?alt=media&token=4cd64235-8f66-41bd-b4d7-9c6e951ed559",1476543600,4,1,1,1,1,1,"A"]]},"db_info":{"headers":["version","last_updated"],"rows":[["29",1476250419]]},"event":{"headers":["_id","parent_fk","unique_id","type","title","description","short_description","location_fk","start_time","end_time","utc_offset","person_demonym","event_demonym"],"rows":[[1,-1,"Reception1","Welcome Reception","Welcome Reception",null,null,10,1476608400,1476615600,540,"",""],[2,-1,"Refreshment1","Refreshment","Refreshment (Coffee & Snacks)",null,null,8,1476661500,1476664200,540,"",""],[3,-1,"Opening","Opening Remarks","Opening Remarks",null,null,6,1476664200,1476666000,540,"",""],[4,-1,"Prenary1","Opening Plenary","Opening Keynote: Takeo Kanade",null,null,6,1476666000,1476669600,540,"",""],[5,-1,"Break1","Coffee Break","Coffee Break (20min)",null,null,8,1476669600,1476670800,540,"",""],[6,-1,"Session1A","Papers","Fabrication",null,null,6,1476670800,1476675600,540,"Chair","Papers"],[7,-1,"Session1B","Papers","Touch and Beyond",null,null,7,1476670800,1476675600,540,"Chair","Papers"],[8,-1,"Lunch1","Lunch","Lunch (Lunchbox provided) ",null,null,5,1476675600,1476680400,540,"Attendee","Lunch"],[9,-1,"Session2A","Papers","TUI",null,null,6,1476680400,1476685200,540,"Chair","Papers"],[10,-1,"Session2B","Papers","Interaction Techniques",null,null,7,1476680400,1476685200,540,"Chair","Papers"],[11,-1,"Break2","Coffee Break","Coffee Break (40min)",null,null,8,1476685200,1476687600,540,"",""],[12,-1,"Session3A","Papers","Touch It, Feel It",null,null,6,1476687600,1476692400,540,"Chair","Papers"],[13,-1,"Session3B","Papers","Dev Tools",null,null,7,1476687600,1476692400,540,"Chair","Papers"],[14,-1,"Reception2","Demo","Demo Reception",null,null,10,1476694800,1476705600,540,"Chair","Demo"],[15,-1,"Refreshment2","Refreshment","Refreshment (Coffee & Snacks)",null,null,8,1476747900,1476750600,540,"",""],[16,-1,"Session4A","Papers","Touch",null,null,6,1476750600,1476755400,540,"Chair","Papers"],[17,-1,"Session4B","Papers","Sensing",null,null,7,1476750600,1476755400,540,"Chair","Papers"],[18,-1,"Break3","Coffee Break","Coffee Break (40min)",null,null,8,1476755400,1476757200,540,"",""],[19,-1,"Session5A","Papers","Viz",null,null,6,1476757200,1476762000,540,"Chair","Papers"],[20,-1,"Session5B","Papers","Physical Displays",null,null,7,1476757200,1476762000,540,"Chair","Papers"],[21,-1,"Lunch2","Lunch","Lunch (Lunchbox provided) ",null,null,5,1476762000,1476766800,540,"Attendee","Lunch"],[22,-1,"Session6A","Papers","Information & Obfusc**ion",null,null,6,1476766800,1476770400,540,"Chair","Papers"],[23,-1,"Session6B","Papers","Video & Audio",null,null,7,1476766800,1476770400,540,"Chair","Papers"],[24,-1,"Break4","Coffee Break","Coffee Break (40min)",null,null,8,1476770400,1476772800,540,"",""],[25,-1,"Session7A","Papers","Fab with New Materials ",null,null,6,1476772800,1476777600,540,"Chair","Papers"],[26,-1,"Session7B","Papers","Text Entry",null,null,7,1476772800,1476777600,540,"Chair","Papers"],[27,-1,"Poster","Poster and SIC","Poster & Student Innovation Competition (Coffee & Snacks)",null,null,8,1476777600,1476786600,540,"",""],[28,-1,"Banquet","Banquet","Banquet",null,null,9,1476788400,1476797400,540,"",""],[29,-1,"Refreshment3","Refreshment","Refreshment (Coffee & Snacks)",null,null,8,1476834300,1476837000,540,"",""],[30,-1,"Session8A","Papers","Crowds",null,null,6,1476837000,1476841800,540,"Chair","Papers"],[31,-1,"Session8B","Papers","Electronics Printing & Prototyping",null,null,7,1476837000,1476841800,540,"Chair","Papers"],[32,-1,"Break5","Coffee Break","Coffee Break (40min)",null,null,5,1476841800,1476843600,540,"",""],[33,-1,"Session9A","Papers","~Reality ",null,null,6,1476843600,1476848400,540,"Chair","Papers"],[34,-1,"Session9B","Papers","Creativity",null,null,7,1476843600,1476848400,540,"Chair","Papers"],[35,-1,"Lunch3","Lunch","Lunch (Lunchbox provided) ",null,null,5,1476848400,1476855000,540,"Attendee","Lunch"],[36,-1,"Meeting","Meeting","Town hall meeting",null,null,7,1476849600,1476853200,540,"",""],[37,-1,"Session10A","Papers","Innovative Interaction ",null,null,6,1476855000,1476859800,540,"Chair","Papers"],[38,-1,"Session10B","Papers","Gesture",null,null,7,1476855000,1476859800,540,"Chair","Papers"],[39,-1,"Break6","Coffee Break","Coffee Break (40min)",null,null,5,1476859800,1476862200,540,"",""],[40,-1,"Plenary2","Closing Prenary","Closing Keynote: Naoto Fukasawa",null,null,6,1476862200,1476865800,540,"",""],[41,-1,"Closing","Closing Ceremony","Closing ceremony",null,null,6,1476865800,1476867600,540,"",""],[42,14,"uistde101","Demo","Luminescent Tentacles: A Scalable SMA Motion Display","The Luminescent Tentacles system is a scalable kinetic surface system for kinetic art, ambient display, and animatronics. The 256 shape-memory alloy actuators react to hand movement by fluid dynamics and Kinect. These actuators behave like waving tentacles of sea anemones under the sea, and the top of the actuator softly glows like a bioluminescent organism. To precisely control a large number of actuators simultaneously, the system utilizes one microcontroller per actuator for distributed processing. In addition, it provides a scalable platform, which can be easily built into various forms.","",10,1476694800,1476705600,540,"",""],[43,14,"uistde102","Demo","Rig Animation with a Tangible and Modular Input Device","We propose a novel approach to digital character animation, combining the benefits of modular and tangible input devices and sophisticated rig animation algorithms. With a symbiotic software and hardware approach, we overcome limitations inherent to all previous tangible devices. It allows users to directly control complex rigs with 5-10 physical controls only. These compact input device configurations - optimized for a specific rig and a set of sample poses - are automatically generated by our algorithm. This avoids oversimplification of the pose space and excessively bulky devices.","",10,1476694800,1476705600,540,"",""],[44,14,"uistde103","Demo","Watch Commander: A Gesture-based Invocation System for Rectangular Smartwatches using B2B-Swipe","We present Watch Commander, a gesture-based invocation system for rectangular smartwatches. Watch Commander allows the user to invoke functions easily and quickly by using Bezel to Bezel-Swipe (B2B-Swipe). This is because B2B-Swipe does not conflict with other swipe gestures such as flick and bezel swipe and can be performed in an eyes-free manner. Moreover, by providing GUIs that display functions assigned with B2B-Swipe, Watch Commander helps the user memorize those functions.","",10,1476694800,1476705600,540,"",""],[45,14,"uistde104","Demo","MagTacS: Delivering Tactile Sensation over an Object","A system that can deliver tactile sensation despite an object existing between an actuator and human was developed. This system composed of a control part, power part, output part, and coil. The control part controls the overall system using a microcontroller. The power part generates electric current to create a magnetic field. The output part delivers high energies to the coil. The coil generates a time-varying magnetic field to induce current flow within the body. Through the tactile sensation recognition test, delivery of tactile sensation was confirmed in the air even an object existed between actuator and human skin.","",10,1476694800,1476705600,540,"",""],[46,14,"uistde106","Demo","floatio: Floating Tangible User Interface Based on Animacy Perception","In this study, we propose floatio: a floating tangible user interface that makes it easy to create a perception of animacy (lifelike movement). It has been pointed out that there are three requirements that make animacy more likely to be perceived: interactivity, irregularities, and automatic movement resisting the force of gravity. Based on these requirements, floatio provides a tangible user interface where a polystyrene ball resembling a pixel is suspended in a stream of air where it can be positioned passively by the user, or autonomously by the system itself. To implement floatio, we developed three mechanisms: a floating field mechanism, a pointer input/output mechanism and a hand-over mechanism. We also measured the precision of the pointer input/output and hand-over mechanisms.","",10,1476694800,1476705600,540,"",""],[47,14,"uistde108","Demo","3D Printed Physical Interfaces that can Extend Touch Devices","We propose a method to create a physical interface that allows touch input to be transferred from an external surface attached to a touch panel. Our technique prints a grid having multiple conductive points using an FDM-based 3D printer. When the user touches the conductive points, touch input is generated. This allows the user to control the touch input at arbitrary locations on an XY plane. By arranging the structure of the conductive wiring inside a physical object, a variety of interfaces can be realized. ","",10,1476694800,1476705600,540,"",""],[48,14,"uistde109","Demo","Thickness Control Technique for Printing Tactile Sheets with Fused Deposition Modeling","We present a printing technique that controls the thickness of objects by increasing and decreasing the amount of material extruded during printing. Using this technique, printers can dynamically control thickness and output thicker objects without a staircase effect. This technique allows users to print aesthetic pattern sheets and objects that are tactile without requiring any new hardware. This extends the capabilities of fused deposition modeling (FDM) 3D printers in a simple way. We describe a method of generating and calculating a movement path for printing tactile sheets, and demonstrate the usage and processing of example objects.","",10,1476694800,1476705600,540,"",""],[49,14,"uistde114","Demo","Sparkle: Towards Haptic Hover-Feedback with Electric Arcs","We demonstrate a method for stimulating the fingertip with touchable electric arcs above a hover sensing input device. We built a hardware platform using a high-voltage resonant transformer for which we control the electric discharge to create in-air haptic feedback up to 4 mm in height, and combined this technology with infrared proximity sensing. Our method is a first step towards supporting novel in-air haptic experiences for hover input that does not require the user to wear haptic feedback stimulators.","",10,1476694800,1476705600,540,"",""],[50,14,"uistde116","Demo","The UIST Video Browser: Creating Shareable Playlists of Video Previews","We introduce the UIST Video Browser which provides a rapid overview of the UIST 30-second video previews, based on the conference schedule. Attendees can see an overview of upcoming talks, search by topic, and create personalized, shareable video playlists that capture the most interesting or relevant papers.","",10,1476694800,1476705600,540,"",""],[51,14,"uistde118","Demo","WithYou: An Interactive Shadowing Coach with Speech Recognition","Speech shadowing, in which the subject listens to native narration sound and tries to repeat it immediately while listening, is a proven way of practicing speaking skills when learning foreign languages. However, since the narration is independent of user's speech, the playback cannot make an adjustment when the learner fails to catch up, and this makes shadowing difficult. We propose WithYou, a system based on Automated Speech Recognition (ASR) that is able to adjust narration playback during a live shadowing speech. WithYou compares the student's live speech with the narration playback to detect shadowing mistakes. In addition, WithYou is able to handle pauses and recognize repetitive phrases in shadowing practice. A user study shows that practicing shadowing with WithYou is easier and more effective compared with conventional methods.     �_ ","",10,1476694800,1476705600,540,"",""],[52,14,"uistde119","Demo","Representing Gaze Direction in Video Communication Using Eye-Shaped Display","A long-standing challenge of video-mediated communication systems is to correctly represent a remote participant???s gaze direction in local environments. To address this problem, we developed a video communication system using an \"eye-shaped display.\" This display is made of an artificial ulexite (TV rock) that is cut into a hemispherical shape, enabling the light from the bottom surface to be projected onto the curved surface. By displaying a simulated iris onto the eye-shaped display, we theorize that our system can represent the gaze direction as accurately as a real human eye. ","",10,1476694800,1476705600,540,"",""],[53,14,"uistde122","Demo","Gushed Diffusers: Fast-moving, Floating, and Lightweight Midair Display","We present a novel method for fast-moving aerial imaging using aerosol-based fog screens. Conventional systems of aerial imaging cannot move fast because they need large and heavy setup. In this study, we propose to add new tradeoffs between limited display time and payloads. This system employ aerosol distribution from off-the-shelf spray as a fog screen that can resist the wind, and have high portability. As application examples, we present wearable application and aerial imaging on objects with high speed movements such as a drone, a radio-controlled model car, and performers. We believe that our study contribute to the exploration of new application areas for fog displays and expand expressions of entertainments and interactivity.","",10,1476694800,1476705600,540,"",""],[54,14,"uistde123","Demo","SoEs: Attachable Augmented Haptic on Gaming Controller for Immersive Interaction","We present SoEs (Sword of Elements), an attachable augmented haptic device for enhancing gaming controller in the immersive first-person game. Generally, Player can easily receive visual and auditory feedback through head-mounted displays (HMD) and headphones from first-person perspective in virtual world. However, the tactile feedback is less than those feedbacks in immersive environment. Although gaming controller, i.e. VIVE or Oculus controller, can provide tactile feedback by some vibration sensors, the haptic feedback is more complicated and various, it includes kinesthesia and cutaneous feedback. Our key idea is to provide a low-cost approach to simulate the haptic feedback of player manipulation in the immersive environment such as striking while the iron is hot which the player could feel the heat and reaction force. Eventually, the game makers could utilize the attachable device into their games for providing haptic feedback.","",10,1476694800,1476705600,540,"",""],[55,14,"uistde125","Demo","LaserStroke: Mid-air Tactile Experiences on Contours Using Indirect Laser Radiation","This demonstration presents a novel form of mid-air tactile display, �_�_emph{LaserStroke}, that makes use of a laser irradiated on the elastic medium attached to the skin. �_ LaserStroke extends a laser device with an orientation control platform and a magnetic tracker so that it can elicit tapping and stroking sensations to a user's palm from a distance. �_ LaserStroke offers unique tactile experiences while a user freely moves his/her hand in midair.","",10,1476694800,1476705600,540,"",""],[56,14,"uistde127","Demo","M.Sketch: Prototyping Tool for Linkage-Based Mechanism Design","We present M.Sketch, a prototyping tool to support non-experts to design and build linkage-based mechanism prototype. It enables users to draw and simulate arbitrary mechanisms as well as to make physical prototype for testing actual movement. Mix of bottom-up and top-down sketching approaches, real-time movement visualization, and functions for digital fabrication can make the users to design the desired mechanism easily and effectively. M.Sketch can be used to design customized products with kinetic movement, such as interactive robot, toys, and sculptures. ","",10,1476694800,1476705600,540,"",""],[57,14,"uistde130","Demo","Depth Based Shadow Pointing Interface for Public Displays","We propose a robust pointing detection with virtual shadow representation for interacting with a public display. Using a depth camera, our shadow is generated by a model with an angled virtual sun light and detects the nearest point as a pointer. The position of the shadow becomes higher when user walks closer, which conveys the notion of correct distance to control the pointer and offers accessibility to the higher area of the display.","",10,1476694800,1476705600,540,"",""],[58,14,"uistde131","Demo","A Tangible Interface to Realize Touch Operations on the Face of a Physical Object","In this paper, we describe a tangible interface that can realize touch operations on a physical object. We printed physical objects that have conductive striped patterns using a multi-material 3D printer. The ExtensionSticker technique allows the user to operate capacitive touch-panel devices by tapping, scrolling, and swiping the physical object. By shaping the structure of conductive wiring inside a physical object, a variety of interfaces can be realized. We examined the conditions for using our proposed method on touch-panel devices.","",10,1476694800,1476705600,540,"",""],[59,14,"uistde133","Demo","AmbioTherm: Simulating Ambient Temperatures and Wind Conditions in VR Environments","As Virtual Reality (VR) experiences become increasingly popular, simulating sensory perceptions of environmental conditions is essential for providing an immersive user experience. In this paper, we present Ambiotherm, a wearable accessory for existing Head Mounted Displays (HMD), which simulates real-world environmental conditions such as ambient temperatures and wind conditions. The system consists of a wearable accessory for the HMD and a mobile application, which generates interactive VR environments and controls the thermal and wind stimuli. The thermal stimulation module is attached to the user's neck while two fans are focused on the user's face to simulate wind conditions. We demonstrate the Ambiotherm system with two VR environments, a desert and a snowy mountain, to showcase the different types of ambient temperatures and wind conditions that can be simulated. Results from initial user experiments show that the participants perceive VR environments to be more immersive when external thermal and wind stimuli are presented as part of the VR experience.","",10,1476694800,1476705600,540,"",""],[60,14,"uistde136","Demo","Wrap & Sense: Grasp Capture by a Band Sensor","This paper proposes a bare hand grasp observation system named Wrap & Sense. We built a band type sensing equipment composed of infrared distance sensors placed in an array. The sensor band is attached to a target object with all sensors directed along the object surface and detects the hand side edge with respect to the object. Assuming type of grasp as ???power grasp???E the whole hand posture can be determined according to the 3D shape of the object. Three types of application are shown as proof-of-concept.","",10,1476694800,1476705600,540,"",""],[61,14,"uistde137","Demo","Facial Expression Mapping inside Head Mounted Display by Embedded Optical Sensors","Head Mounted Display (HMD) provides an immersive ex-perience in virtual environments for various purposes such as for games and communication. However, it is difficult to capture facial expression in a HMD-based virtual environ-ment because the upper half of user???s face is covered up by the HMD. In this paper, we propose a facial expression mapping technology between user and a virtual avatar using embedded optical sensors and machine learning. The distance between each sensor and surface of the face is measured by the optical sensors that are attached inside the HMD. Our system learns the sensor values of each facial expression by neural network and creates a classifier to estimate the current facial expression. �_  �_ ","",10,1476694800,1476705600,540,"",""],[62,14,"uistde138","Demo","An Input Switching Interface Using Carbon Copy Metaphor","This paper proposes a novel input technique that aims to switch between relative and absolute coordinates input methods seamlessly based on the \"carbon copy`` metaphor for track-pads.","",10,1476694800,1476705600,540,"",""],[63,14,"uistde139","Demo","Designing a Haptic Feedback System for Hearing-Impaired to Experience Tap Dance","In this study, we have designed a system to enable hearing-impaired to enjoy the performance of tap dancers. This system transfers the haptic sensation of tap dancing from the stage to the audience and helps hearing-impaired people enjoy the vibration of the taps even if they cannot hear the sound. We organized an event to verify the effectiveness of the system. To do this, we collaborated with a tap dance unit and science museum. We found that our system succeeded in helping the tap dancers share the fun and enjoyment of dance with the audience comprising people with hearing disabilities.","",10,1476694800,1476705600,540,"",""],[64,14,"uistde141","Demo","Ballumiere: Real-Time Tracking and Projection for High-Speed Moving Balls","Projection onto moving objects has a serious slipping problem due to delay between tracking and projection. We propose a new method to overcome the delay problem, and we succeed in increasing the accuracy of projection. We present Ballumiere as a demo for projection to volleyballs and juggling balls.","",10,1476694800,1476705600,540,"",""],[65,14,"uistde142","Demo","ScalableBody : A Telepresence Robot Supporting Socially Acceptable Interactions and Human Augmentation through Vertical Actuation","Most telepresence robots have a fixed-size body, and are unable to change the camera or display position. �_ Therefore, although making eye contact is important in human expression, current fixed-size telepresence robots fail to achieve this. �_  �_ We propose a novel telepresence robot called ScalableBody, which enables users to make eye contact during conversations by changing its height. �_ ScalableBody extends its body to modify the position of its camera or display. �_ This approach provides eye contacts in remote conversations, thus creating almost same situation when the remote and local users make conversation like a real meeting. �_ As for the remote users, this approach also enables them to experience having a conversation from different heights, such as being a giant or a dwarf. �_ This technique extends the possibilities of remote communication by telepresence robots.","",10,1476694800,1476705600,540,"",""],[66,14,"uistde143","Demo","MlioLight: Multi-Layered Image Overlay using Multiple Flashlight Devices","We propose a technique that overlays natural images on the real world using the information from  multiple flashlight devices. We focus on finding areas of overlapping lights in a multiple light-source scenario and overlaying multi-layered information on a real world object in these areas.In order to mix multiple images, we developed a light identification and overlapping area detection technique using rapid synchronization between high-speed cameras and multiple light devices.In this paper, we describe the concept of our system and a prototype implementation.We also describe two different applications.","",10,1476694800,1476705600,540,"",""],[67,14,"uistde145","Demo","Polyspector??��: An Interactive Visualization Platform Optimized for Visual Analysis of Big Data","With the advent of the ???big data???Eera, there are unprecedented opportunities and challenges to explore complex and large datasets. In the paper, we introduce Polyspector??��, a web-based interactive visualization platform optimized for interactive visual analysis with two distinguishing features. Firstly, a visualization-specific database engine based on pixel-aware aggregation is implemented to generate views of hundreds of millions of data items within a second even with an off-the-shelf PC. Secondly, a novel deep-linking mechanism, combined with the pixel-aware aggregation, is exploited to realize interactive visual analysis interfaces such as zooming, overview + detail, context + focus etc.","",10,1476694800,1476705600,540,"",""],[68,14,"uistde148","Demo","Hand Gesture and On-body Touch Recognition by Active Acoustic Sensing throughout the Human Body","In this paper, we present a novel acoustic sensing technique that recognizes two convenient input actions: hand gestures and on-body touch. We achieved them by observing the frequency spectrum of the wave propagated in the body, around the periphery of the wrist. Our approach can recognize hand gestures and on-body touch concurrently in real-time and is expected to obtain rich input variations by combining them. We conducted a user study that showed classification accuracy of 97%, 96%, and 97% for hand gestures, touches on the forearm, and touches on the back of the hand. ","",10,1476694800,1476705600,540,"",""],[69,14,"uistde152","Demo","Wolverine: A Wearable Haptic Interface for Grasping in VR","The Wolverine is a mobile, wearable haptic device designed for simulating the grasping of rigid objects in virtual environment. In contrast to prior work on force feedback gloves, we focus on creating a low cost, lightweight, and wireless device that renders a force directly between the thumb and three fingers to simulate objects held in pad opposition type grasps. Leveraging low-power brake-based locking sliders, the system can withstand over 100N of force between each finger and the thumb, and only consumes 2.78 {�_�_textmu Wh}(10 mJ) for each braking interaction. Integrated sensors are used both for feedback control and user input: time-of-flight sensors provide the position of each finger and an IMU provides overall orientation tracking. This design enables us to use the device for roughly 6 hours with 5500 full fingered grasping events. The total weight is 55g including a 350 mAh battery.","",10,1476694800,1476705600,540,"",""],[70,14,"uistde154","Demo","Phones on Wheels: Exploring Interaction for Smartphones with Kinetic Capabilities","This paper introduces novel interaction and applications using smartphones with kinetic capabilities. �_ We develop an accessory module with robot wheels for a smartphone. �_ With this module, the smartphone can move in a linear direction or rotate with sufficient power. �_ The module also includes rotary encoders, allowing us to use the wheels as an input modality. �_ We demonstrate a series of novel mobile interaction for mobile devices with kinetic capabilities through three applications.","",10,1476694800,1476705600,540,"",""],[71,14,"uistde155","Demo","Applications of Switchable Permanent Magnetic Actuators in Shape Change and Tactile Display","Systems realizing shape change and tactile display remain hindered by the power, cost, and size limitations of current actuation technology. We describe and evaluate a novel use of switchable permanent magnets as a bistable actuator for haptic feedback which draws power only when switching states. Because of their efficiency, low cost, and small size, these actuators show promise in realizing tactile display within mobile, wearable, and embedded systems. We present several applications demonstrating potential uses in the mobile, automotive, and desktop computing domains, and perform a technical evaluation of the actuators used in these systems.","",10,1476694800,1476705600,540,"",""],[72,14,"uistde158","Demo","Virtual Sweet: Simulating Sweet Sensation Using Thermal Stimulation on the Tip of the Tongue","Being a pleasurable sensation, sweetness is recognized as the most preferred sensation among the five primary taste sensations. In this paper, we present a novel method to virtually simulate the sensation of sweetness by applying thermal stimulation to the tip of the human tongue. To digitally simulate the sensation of sweetness, the system delivers rapid heating and cooling stimuli to the tongue via a 2x2 grid of Peltier elements. To achieve distinct, controlled, and synchronized temperature variations in the stimuli, a control module is used to regulate each of the Peltier elements. Results from our preliminary experiments suggest that the participants were able to perceive mild sweetness on the tip of their tongue while using the proposed system. ","",10,1476694800,1476705600,540,"",""],[73,14,"uistde159","Demo","Fitter: A System for Easily Printing Objects that Fit Real Objects","When printing both self-making and existing 3D models, users often create models to fit to a real object within it. Fitting models to the size of a real object is a delicate problem. To address it, we present a concept to capture the size of a real object, create or modify a model that conforms to the captured image, and print the model on the spot. We create a 3D printer to realize this concept by installing a touch panel display in the build plate system. In this paper, we focus on creating containers that fit accessories. We create containers for a pair of scissors, a smart watch, a drone, a pair of glasses, and a pen holder.","",10,1476694800,1476705600,540,"",""],[74,14,"uistde160","Demo","RunPlay: Action Recognition Using Wearable Device Apply on Parkour Game","In this paper, we present an action recognition system which consists of pressure insoles, with 16 pressure sensors, and an inertial measurement unit. By analysing the data measured from these sensors, we are able to recognised several human activities. In this circumstance, we focus on the detection of jumping, squatting, moving left and right. We also designed a parkour game on a mobile device to demonstrate the in-game control of an avatar by human action.   ","",10,1476694800,1476705600,540,"",""],[75,14,"uistde162","Demo","Music Composition with Recommendation","Creating a piece of music requires deep knowledge of composition, and is time-consuming even for experts. Algorithmic composition systems can generate pieces in an existing style. However, these systems are not interactive. Therefore, it is difficult for them to express the user???s intention.  �_  �_ We propose a system that recommends a continuation melody in accordance with a melody expressed by the user. Recommendation uses the style of the piece of the composer, thus users give the system a piece of the style in which they want to compose. With this system, users can compose pieces tailored to their needs, and composers can get assistance with composition. ","",10,1476694800,1476705600,540,"",""],[76,14,"uistde163","Demo","waveSense: Ultra Low Power Gesture Sensing Based on Selective Volumetric Illumination","We present waveSense, a low power hand gestures recogni- tion system suitable for mobile and wearable devices. A novel Selective Volumetric Illumination (SVI) approach using off-the-shelf infrared (IR) emitters and non-focused IR sensors were introduced to achieve the power efficiency. Our current implementation consumes 8.65mW while sensing hand gestures within 60cm radius from the sensors. In this demo, we introduce the concept and the theoretical background of waveSense, details of the prototype implementation, and application possibilities.","",10,1476694800,1476705600,540,"",""],[77,14,"uistde164","Demo","Expanding the Field-of-View of Head-Mounted Displays with Peripheral Blurred Images","Head-mounted displays are rapidly becoming popular. Field-of-view is one of the key parameters of head-mounted displays, because a wider field-of-view gives higher presence and immersion in the virtual environment. However, wider field-of-view often increase device cost and weight because it needs complicated optics or expensive modules such as multi high-resolution displays or complex lenses. This paper proposes a method that expands the field-of-view by using two kinds of lenses with different levels of magnification. The principle of the proposed method is that Fresnel lenses with high magnification surround convex lenses to fill the peripheral vision with a blurred image. The proposed method doesn???t need complicated optics, and is advantageous in terms of device cost and weight, because only two additional Fresnel lenses are necessary. We implement a prototype and confirm that the Fresnel lenses fill the peripheral with a blurred image, and effectively expand the field-of-view.","",10,1476694800,1476705600,540,"",""],[78,14,"uistde165","Demo","Reconstruction of Scene from Multiple Sketches","This paper discusses the feasibility of extension of expressive style with multiple 3D sketches drawn by a sketching tool that enables its users to draw and paint on 3D structured surfaces. Users of our proposed system take a picture of target objects and sketch with reference to the taken picture. They can not only sketch on the pictures but can also change their viewpoint of the sketched environment, since the system captures 3D structure by using a depth sensor as well as RGB data.  Trial usage of the system shows that our users can rapidly extract their target objects/space and extend their ideas by taking pictures and drawing/painting on them. This paper presents examples of system usage, and discusses the feasibility of extension of sketches.","",10,1476694800,1476705600,540,"",""],[79,14,"uistde166","Demo","2.5 Dimensional Panoramic Viewing Technique utilizing a Cylindrical Mirror Widget","We propose a new map based scene viewing system, which applies the technique of Anamorphosis, mapping a 2D display onto a cylindrical mirror.In this system, a distorted scene image is shown on a flat panel display or tabletop surface. When a user places the cylindrical mirror on the display, the original image appears on the cylindrical mirror. The image can be seen as if it is a stereoscopic image.By detecting the position and rotation of the cylinder, the system provides interaction between the user and images on the cylinder. ","",10,1476694800,1476705600,540,"",""],[80,14,"uistde167","Demo","ExtendedHand on Wheelchair","In this paper, we present a novel welfare system which utilizes a spatial augmented reality technique. Hand is a crucial component in human-human communication. For example, we can intuitively indicate an object or place by reaching and pointing it to nearby partners. Unfortunately, for wheelchair users, such communication is often limited because their reaching ranges are narrow, and moving their bodies to the target is tiresome. To solve this issue, we propose a novel wheelchair system on which a battery-powered mobile projector is mounted. A user manipulates the projected virtual hand as an extension of the real one using a touch panel equipped on an armrest of the wheelchair. We implement our proposed system and demonstrate the effectiveness.","",10,1476694800,1476705600,540,"",""],[81,14,"uistde172","Demo","Synesthesia Suit","We propose the full body suit that presents an immersive experience in Virtual Reality. The suit has 24 tactile actuators and each of this shows well-designed tactile sensation within virtual reality contents. The suit was designed easy-to-wear for all person regardless of body size. And we made software that controls 24ch tactile signals. We present the synesthesia experience integrated light, sound and sense of touch. We describe a suit design, a haptic controller development and haptic contents design and depict future experience toward designing embodied entertainment including haptic sensation.","",10,1476694800,1476705600,540,"",""],[82,14,"uistde173","Demo","Interaction Technique Using Acoustic Sensing for Different Squeak Sounds Caused by Number of Rubbing Fingers","Various studies have been conducted for developing interaction techniques in a smart house. Some of our previous studies [1, 2] focused on bathrooms and we converted an existing normal bathtub system into a user interface by using embedded sensors. A system called Bathcratch [2] detects squeak sounds by rubbing on a bathtub edge. To generate squeaks, it requires some conditions to cause the Stick-slip phenomenon. A bathtub has a smooth surface, and water can cause the phenomenon with human skins. Bathcratch uses it as an interaction technique to play DJ-scratching.  �_ Here, we extended the interaction technique using squeaks to recognize rubbing states, rubbing events including sequence, and the difference between squeaks caused by the number of fingers. This can be used in various wet environments including kitchen, washbowls in a restroom, swimming pool, and spa. This paper describes the method and its performance for identifying the number of rubbing fingers by using frequency analysis. In addition, we illustrate some smart home applications by using the proposed technique.","",10,1476694800,1476705600,540,"",""],[83,14,"uistde174","Demo","Fluxa: Body Movements as a Social Display","This paper presents Fluxa, a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body.  When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a wearable display to foster social interactions. It can be used to enhance existing social gestures such as hand-waving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a self-expression device that generates images while dancing. We discuss the advantages of Fluxa: a display size that could be much larger than the device itself, a semi-transparent display that allows users and others to see though it and promotes social interaction.","",10,1476694800,1476705600,540,"",""],[84,14,"uistde176","Demo","WhammyPhone: Exploring Tangible Audio Manipulation Using Bend Input on a Flexible Smartphone","We present WhammyPhone, a novel audio interface that supports physical manipulation of digital audio through bend gestures. WhammyPhone combines a high-resolution flexible display, bend sensors, and a set of intuitive interaction techniques that enable novice users to manipulate sound in a tangible fashion. With WhammyPhone, bend gestures can control both discrete (e.g. triggering a note) and continuous parameters (e.g. pitch bend). We showcase application scenarios that leverage the unique input modalities of WhammyPhone and discuss its potential for digital audio manipulation.","",10,1476694800,1476705600,540,"",""],[85,14,"uistde177","Demo","UnlimitedHand: Input and Output Hand Gestures with Less Calibration Time","Numerous devices that either track hand gestures or provide haptic feedback have been developed with the aim of manipulating objects within Virtual Reality(VR) and Augmented Reality(AR) environments. However, these devices implement lengthy calibration processes to ease out individual differences. �_ In this research, a wearable device that simultaneously recognizes hand gestures and outputs haptic feedback: UnlimitedHand is suggested. Photo-reflectors are placed over specific muscle groups on the forearm to read in hand gestures. For output, electrodes are placed over the same muscles to control the user's hand movements. Both sensors and electrodes target main muscle groups responsible for moving the hand. Since the positions of these muscle groups are common between humans, UnlimitedHand is able to reduce the time spent on performing calibration.","",10,1476694800,1476705600,540,"",""],[86,27,"uistp103","Poster","Orchestrated Informal Care Coordination: Designing a Connected Network of Tools in Support of Collective Care Activities for Informal Caregivers","Often, family caregivers experience difficulties in coordinating older adults�f health care because it requires not only a lot of time but also a diverse set of responsibilities to coordinate care for their loved ones. While many can reduce their individual burden by sharing care tasks with other family members, there are still many challenges to overcome in maintaining the quality of care when they work together. As they increase their informal care network, it becomes more difficult for them to stay informed and coordinated. Coordination breakdowns caused by having multiple caregivers who are cooperating to care for the same care recipient result in reduced quality of care. I explored opportunities for �gInternet of Things (IoT)�h technologies to help informal caregivers better coordinate and communicate care with each other for their loved ones. Based on identified design opportunities, I propose the concept of CareBot, a smart home platform consisting of interactive tools in support of collective care activities of family caregivers. �_ ","",8,1476777600,1476786600,540,"",""],[87,27,"uistp109","Poster","Switch++: An Output Device of the Switches by the Finger Gestures","Regarding human-machine-interfaces, switches have not changed significantly despite the machines themselves evolving constantly. In this paper, we propose a new method of operability for devices by providing multiple switches dynamically, and users choose the switch that has the functionality that they want to use. Switch++ senses the mental model of the operating sensation of switches against the user�fs finger gestures and changes the shape of the switch and its affordances accordingly. We design the interface based on the raw data.","",8,1476777600,1476786600,540,"",""],[88,27,"uistp110","Poster","Transparent Reality: Using Eye Gaze Focus Depth as Interaction Modality","We present a novel, eye gaze based interaction technique, using focus depth as an input modality for virtual reality (VR) applications. We also show custom hardware prototype implementation. Comparing the focus depth based interaction to a scroll wheel interface, we find no statistically significant difference in performance (the focus depth works slightly better) and    a subjective preference of the users in a user study with 10 participants playing a simple VR game. This indicates that it is a suitable interface modality that should be further explored. Finally, we give some application scenarios and guidelines for using focus depth interactions in VR applications.","",8,1476777600,1476786600,540,"",""],[89,27,"uistp114","Poster","Toward a Compact Device to Interact with a Capacitive Touch Screen","Capacitive touch screens �_ are widely used in various products.  �_ Touch screens have an advantage that an input system �_ and output system can be integrated into a single module. �_ We consider this advantage could make it possible to �_ realize a new universal interface for both human-to-machine (H2M) and �_ machine-to-machine (M2M). For a M2M interface, �_ some sort of method to simulate finger touching is needed. �_ Therefore, we propose an alternative method to interact with a touch screen �_ using two electrical approaches. �_ Our proposal is effective in  �_ automating touch screen operations, modality conversion device �_ for people with disabilities, and so on. �_ We assembled a prototype to confirm the principle to �_ control a touch screen with the electrical methods. �_ We believe that our proposal will complement the weakness of touch screens �_ and expand their possibility. �_ ","",8,1476777600,1476786600,540,"",""],[90,27,"uistp116","Poster","Initial Trials of ofxEpilog: From Real Time Operation to Dynamic Focus of Epilog Laser Cutter","This paper describes ofxEpilog which enable people to control a laser cutter of Epilog in real time. ofxEpilog is an addon of openFrameworks, an open source C++ toolkit for creative coding. With the addon, people could directly send their image object to a laser cutter through Ethernet. By alternating the generation and transmission of the command of cutting, the addon could sequentially control a laser cutter in real time. This paper introduces our initial trials of ofxEpilog with a real time operation (A), dynamic focus (z-axis) control with a given 3D object (B), and a scanned 3D object (C). Technical limitations and our upcoming challenges are also discussed.","",8,1476777600,1476786600,540,"",""],[91,27,"uistp122","Poster","Design and Evaluation of EdgeWrite Alphabets for Round Face Smartwatches","This study presents a project aimed at designing and evaluating a unistroke gesture set of alphanumeric characters targeting round-face smartwatches. �_ We conducted a user study with 10 participants to generate the basic gesture design for 40 characters. �_ For each character, we measured the preference and agreement scores and uncovered any challenges faced in designing unistroke gestures for round-face smartwatches. �_ We developed a gesture recognizer using machine learning, which used a backpropagation mechanism to evaluate the designed gestures. �_ Using the gesture recognizer, we collected 80,000 gesture data, and evaluated them with 5-fold cross-validation. �_ The obtained mean recognition rate was 92.14%.","",8,1476777600,1476786600,540,"",""],[92,27,"uistp125","Poster","Touchscreen Overlay Augmented with the Stick-Slip Phenomenon to Generate Kinetic Energy","Kinesthetic feedback requires linkage-based high-powered multi-dimensional manipulators, which are currently not possible to integrate with mobile devices. To overcome this challenge, we developed a novel system that can utilize a wide range of actuation components and apply various techniques to optimize stick-slip motion of a tangible object on a display surface. The current setup demonstrates how it may be possible to generate directional forces on an interactive display in order to move a linkage-free stylus over a touchscreen in a fully controlled and efficient manner. The technology described in this research opens up new possibilities for interacting with displays and tangible surfaces such as continuously supervised learning; active feed-forward systems as well as dynamic gaming environments that predict user behavior and are able modify and physically react to human input at real-time.","",8,1476777600,1476786600,540,"",""],[93,27,"uistp132","Poster","Histogram: Spatiotemporal Photo-Displaying Interface","As the smartphone has become more widely available, we easily take photos and upload them online to share with others. Photographs are abundant, but they are not used properly, even though they provide meaningful information about the social scenes of our daily lives. To address this issue, Histogram was created as a new interface for displaying and sharing location-related photographs chronologically to trace the changes in a location. The prototype of this system is mobile-optimized to encourage users to easily upload photos with their smartphones, so that the system can be run through social cooperative work.","",8,1476777600,1476786600,540,"",""],[94,27,"uistp136","Poster","AquaCAVE: Augmented Swimming Environment with Immersive Surround-Screen Virtual Reality","AquaCAVE is a system for enhancing the swimming experience. Although swimming is considered to be one of the best exercises to maintain our health, swimming in a pool is normally monotonous; thus, maintaining its motivation is sometimes difficult. AquaCAVE is a computer-augmented swimming pool with rear-projection acrylic walls that surround a swimmer, providing a CAVE-like immersive stereoscopic projection environment. The swimmer wears goggles with liquid-crystal display (LCD) shutter glasses, and cameras installed in the pool tracks swimmer's head position. Swimmers can be immersed into synthetic scenes such as coral reefs, outer space, or any other computer generated environments. The system can also provide swimming training with projections such as record lines and swimming forms as 3D virtual characters in the 3D space.","",8,1476777600,1476786600,540,"",""],[95,27,"uistp137","Poster","Partial Bookmarking: A Structure-independent Mechanism of Transclusion for a Portion of any Web Page","A novel mechanism of transclusion for collecting and producing information on the Web, named partial bookmarking, is proposed. Partial bookmarking allows a user to collect portions of any web page by making it able to use for a spatial hypertext, like a web document element, without the need to duplicate its contents. Whereas the previous studies involving transclusion required pre-designed linkable objects, such as XML elements or HTML objects, partial bookmarking does not rely on any document structure. To accomplish partial bookmarking, we enhanced a conventional web browser with multiple tabs by introducing the technology of mirroring to display only a portion of a web page appropriately while factoring in potential copyright issues.","",8,1476777600,1476786600,540,"",""],[96,27,"uistp138","Poster","Thermocons: Evaluating the Thermal Haptic Perception of the Forehead","Thermocons describes our work in progress for evaluating thermal haptic feedback on the forehead as a viable feedback modality for integration with head mounted devices. The purpose was to identify the thermal perception for simultaneous feedback at three locations of the forehead. We provided hot-only, cold-only and hot/cold-mixed thermal stimulations at these location to identify the sensitivity for accurate perception. Our evaluation with 9 participants indicated that perceiving cold-only stimulations were significantly better with an accuracy of 88�_�_%. The perception accuracy for hot-only and hot�_�_/cold-mixed stimulations were 66�_�_% and 65�_�_% respectively.  �_ ","",8,1476777600,1476786600,540,"",""],[97,27,"uistp141","Poster","Sidetap & Slingshot Gestures On Unmodified Smartwatches","We present a technique for detecting gestures on the edge of an unmodified smartwatch. We demonstrate two exemplary gestures, i) Sidetap - tapping on any side and ii) Slingshot - pressing on the edge and then releasing quickly. Our technique is lightweight, as it relies on measuring the data from the internal Inertial measurement unit (IMU) only. With these two gestures, we expand the input expressiveness of a smartwatch, allowing users to use intuitive gestures with natural tactile feedback, e.g., for the rapid navigation of a long list of items with a tap, or act as shortcut commands to launch applications. It can also allow for eyes-free interaction or subtle interaction where visual attention is not available.","",8,1476777600,1476786600,540,"",""],[98,27,"uistp148","Poster","Developing fMRI-Compatible Interaction Systems through Air Pressure","We leverage the use of air pressure to expand the interaction space within fMRI (functional magnetic resonance imaging). We present three example applications that are not previously possible in conventional fMRI interaction devices: 1) pedal interface that can record continuous pressure value pressed by users, 2) wrist tactile interface that can provide various tactile patterns or stimuli, 3) adjustable resistance joystick that can provide feedback through different resistance levels. Our work shows that the use of air pressure can enable new research opportunities for fMRI researchers.","",8,1476777600,1476786600,540,"",""],[99,27,"uistp151","Poster","OmniEyeball: Spherical Display Embedded With Omnidirectional Camera Using Dynamic Spherical Mapping","Recently, 360-degree panorama and spherical displays have received more and more attention due to their unique panoramic properties. Compared with existing works, we plan to utilize omnidirectional cameras in our spherical display system to enable omnidirectional panoramic image as input and output. In our work, we present a novel movable spherical display embedded with omnidirectional cameras. Our system can use embedded cameras to shoot 360-degree panoramic video and project the live stream from its cameras onto its spherical display in real time. �_ In addition, we implemented an approach to achieve the dynamic spherical projection mapping in order to project to moving spherical devices. We have also been creating applications utilizing system's features by using 360-degree panoramic image as input and output.","",8,1476777600,1476786600,540,"",""],[100,27,"uistp153","Poster","Estimating contact force of fingertip and providing tactile feedback simultaneously","This study proposes a method for estimating the contact force �_ of the fingertip by inputting vibrations actively. The use of active bone-conducted sound sensing has been limited to estimating the joint angle �_ of the elbow and the finger. We applied it �_ to the method for estimating the contact force of the fingertip. Unlike related works, it is not necessary to mount the device on a fingertip, and tactile feedback is enabled using tangible vibrations. �_ ","",8,1476777600,1476786600,540,"",""],[101,27,"uistp156","Poster","Activity-Aware Video Stabilization for BallCam","We present a video stabilization algorithm for ball camera systems that undergo extreme egomotion during sports play. In particular, we focus on the BallCam system which is an American football embedded with an action camera at the tip of the ball. We propose an activity-aware video stabilization algorithm which is able to understand the current activity of the BallCam, which uses estimated activity labels to inform a robust video stabilization algorithm. Activity recognition is performed with a deep convolutional neural network, which uses optical flow. �_ ","",8,1476777600,1476786600,540,"",""],[102,27,"uistp158","Poster","Study on Control Method of Virtual Food Texture by Electrical Muscle Stimulation","We propose Electric Food Texture System, which can present virtual food texture such as hardness and elasticity by electrical muscle stimulation (EMS) to the masseter muscle. �_ In our previous study, we investigated the feasibility to detect user's bite with a photoreflector and that to construct database of food texture with electromyography sensors. �_ In this paper, we investigated the feasibility to control virtual food texture by EMS. �_ We conducted an experiment to reveal the relationship of the parameters of EMS and those of virtual food texture. �_ The experimental results show that the higher strength of EMS is, the harder virtual food texture is, and the longer duration of EMS is, the more elastic virtual food texture is. �_ ","",8,1476777600,1476786600,540,"",""],[103,27,"uistp159","Poster","Uniformity Based Haptic Alert Network","We experience haptic feedback on a wide variety of devices in the modern day, including cellphones, tablets, and smartwatches. However haptic alerts can quickly become disruptive rather than helpful to a user when multiple devices are providing feedback simultaneously or consecutively. Thus in this paper, we propose an intercommunicating, turn-based local network between a user's devices. This will allow a guaranteed minimal time span between device alerts. Additionally, when multiple devices provide a notification-based haptic alert, devices often produce different feedback due to the varying materials they are placed on. To address this, our framework allows devices to self-regulate their levels of haptic responses based on the material density of the surface they are placed on. This allows the framework to enforce a uniform level of haptic feedback across all the surface-device combinations. Finally, we will also utilize this common network to eliminate redundant alerts across devices.","",8,1476777600,1476786600,540,"",""],[104,27,"uistp161","Poster","Flying User Interface","This paper describes a special type of drone called \"Flying User Interface\", comprised of a robotic projector-camera system, an onboard digital computer connected with the Internet, sensors, and a hardware interface capable of sticking to any surface such as wall, ceilings, etc. Computer further consists of other subsystems, devices, and sensors such as accelerometer, compass, gyroscope, flashlight, etc. Drone flies from one place to another, detects a surface, and attaches itself to it. After a successful attachment, the device stops all its rotators; it then projects or augments images, information, and user interfaces on nearby surfaces and walls. User interface may contain applications, information about object being augmented and information from Internet. User can interact with user-interface using commands and gestures such as hand, body, feet, voice, etc.","",8,1476777600,1476786600,540,"",""],[105,27,"uistp162","Poster","Prevention of Unintentional Input While Using Wrist Rotation for Device Configuration","We describe the design of the safeguard interface that helps users avoid unintentional input while using wrist rotation. When configuring the parameters of various devices, our interface helps reduce the chance of making accidental changes by delaying the result of input and allowing the users to make deliberate attempt to change the parameters to their desired value. We evaluated our methods with a set of user experience and found that our methods were more preferred when the end-results of configurational changes of the devices become more critical and can cause irreversible damage.","",8,1476777600,1476786600,540,"",""],[106,27,"uistp167","Poster","Peripersonal Space in Virtual Reality: Navigating 3D Space with Different Perspectives","We introduce the concept of �gperipersonal space�h of an avatar in 3D virtual reality and discuss how it plays an important role on 3D navigation with different perspectives. By analyzing the eye-gaze data of avatar-based navigation with first-person perspective and third-person perspective, we examine the effects of an avatar�fs peripersonal space on the users�f perceptual scopes within 3D virtual environments. We propose that manipulating peripersonal space of an avatar with various perspectives has the immediate effects on the users�f scopes of perception as well as the patterns of attentional capture. This study provides a helpful guideline for designing more effective navigation system with an avatar in 3D virtual environment.","",8,1476777600,1476786600,540,"",""],[107,27,"uistp171","Poster","A Novel Real Time Monitor System of 3D Printing Layers for Better Slicing Parameter Setting","We proposed a novel real time monitor system of 3D printer with dual cameras, which capture and reconstruct the printed result layer by layer. With the reconstructed image, we can apply computer vision technique to evaluate the difference with the ideal path generate by G-code. The difference gives us clues to classify which might be the possible factor of the result. Hence we can produce advice to user for better slicing parameter settings. We believe that this system can give helps to beginner or users of 3D printer that struggle in parameter settings in the future.","",8,1476777600,1476786600,540,"",""],[108,27,"uistp172","Poster","Hilbert Curves: A Tool for Resolution Independent Haptic Texture","Haptic systems usually stimulate the kinesthetic aspects of the sense of touch, i.e. force feedback systems. But more and more devices aim to stimulate the cutaneous part of the sense of touch to reproduce more complex tactile sensations. To do so, they stimulate one�fs fingertip in different locations, usually in the fashion of a matrix pattern. In this paper we investigate the new possibilities that are offered by such a framework and present an ongoing project that investigates the benefits of Hilbert curves to display resolution independent mid-air haptic textures in comparison with other implementation approaches. ","",8,1476777600,1476786600,540,"",""],[109,27,"uistp173","Poster","Friend*Chip: A Bracelet with Digital Pet for Socially Inclusive Games for Children","Learning in groups have different potential benefits for children. They have the opportunity to solve problems together, to share experiences and to develop social skills. However, from teachers point of view, creating a safe and inclusive positive environment for children is not an simple task since each child has differences that represent a challenge for implementing effectively group dynamics. The focus of this work is the design of a system that motivates children to approach to others and create opportunities of social interaction. The system creates a fun and enjoyable situation that is always supervised by the teacher, who can monitor and change the group dynamics at any moment during the activity.","",8,1476777600,1476786600,540,"",""],[110,27,"uistp176","Poster","NeverMind: Using Augmented Reality for Memorization","NeverMind is an interface and application designed to support human memory. We combine the memory palace memorization method with augmented reality technology to create a tool to help anyone memorize more effectively. Preliminary experiments show that content memorized with NeverMind remains longer in memory compared to general memorization techniques. With this project, we hope to make the memory palace method accessible to novices and demonstrate one way augmented reality can support learning. �_ ","",8,1476777600,1476786600,540,"",""],[111,27,"uistp184","Poster","Analysis of Sequential Tasks in Use  Context of Mobile Apps","Most of the work on context-aware systems has focused on the context of time, location, and activity. Previous studies on the context flow have been primarily conducted on a qualitative basis. This paper proposes a new approach from a quantitative perspective. We gathered the data from an automated task service, �gIf This Then That (IFTTT)�h, and analyzed the sequential tasks in terms of event occurrence in smart devices through association rule mining. We found out three consecutive tasks in cross-application. The results of analysis have potential to find hidden use patterns as telling what kinds of services and channels are associated with each other. The findings provide some insights on the development of design guidelines for context-aware services.","",8,1476777600,1476786600,540,"",""],[112,27,"uistp187","Poster","Towards understanding collaboration around interactive surfaces: Exploring joint visual attention","In this abstract, we present a novel method for exploring the visual behavior of multiple users engaged in a collaborative task around an interactive surface. The proposed method synchronizes input from multiple eye trackers, describes the visual behavior of individual users over time, and identifies joint attention across multiple users. We applied this method to analyze the visual behavior of four users collaborating using a large-scale multi-touch tabletop.","",8,1476777600,1476786600,540,"",""],[113,27,"uistp193","Poster","Resolving Spatial Variation And Allowing Spectator Participation In Multiplayer VR","Multiplayer virtual reality (VR) games introduce the problem of variations in the physical size and shape of each user's space for mapping into a shared virtual space. We propose an asymmetric approach to solve the spatial variation problem, by allowing people to choose roles based on the size of their space. We demonstrate this concept through the implementation of a virtual snowball fight where players can choose from multiple roles, namely the shooter, the target, or an onlooker depending on whether the game is played remotely or together in one large space. In the co-located version, the target stands behind an actuated cardboard fort that responds to events in VR, providing non-VR spectators a way to participate in the experience. During preliminary deployment, users showed extremely positive reactions and the spectators were thrilled.","",8,1476777600,1476786600,540,"",""],[114,27,"uistp195","Poster","OctaRing: Examining Pressure-Sensitive Multi-Touch Input on a Finger Ring Device","In this paper, we introduce OctaRing, an octagon-shaped finger ring device that facilitates pressure-sensitive multi- touch gestures. To explore the feasibility of its prototype, we conducted an experiment and investigated users�f sensorimotor skills in exerting different levels of pressure on the ring with more than one finger. The results of the experiment indicate that users are comfortable with the two-finger touch configuration with two levels of pressure. Based on this result, future work will explore novel gestures involving a finger ring device.","",8,1476777600,1476786600,540,"",""],[115,4,"keynote1","Keynote","Smart Headlight: An Application of Projector-Camera Vision","A projector manipulates outgoing light rays, while a camera records incoming ones. Combining these optically inverse devices, especially in a coaxial manner, creates the possibility of a new computer-vision technology. The �gSmart Headlight,�h currently under development at Carnegie Mellon�fs Robotics Institute, is one example:   a device that can �gerase�h raindrops or snowflakes from a driver�fs sight, allowing for continuous use of the �ghigh beams�h mode while not causing glare against oncoming drivers, and enhance the appearance of important objects, such as pedestrians.  In that sense, it constitutes a �ggenuine�h augmented reality, manipulating the reality for how it appears to a viewer, rather than merely overlaying objects on the image of the reality. This talk will present the state of the Smart Headlight project and discuss further possible applications of projector-camera systems.","",6,1476666000,1476669600,540,"Author"],[116,6,"uist4731","Full","Mobile Fabrication","We present an exploration into the future of fabrication, in particular the vision of mobile fabrication, which we define as “personal fabrication on the go�. We explore this vision with two surveys, two simple hardware prototypes, matching custom apps that provide users with access to a solution database, custom fabrication processes we designed specifically for these devices, and a user study conducted in situ on metro trains. Our findings suggest that mobile fabrication is a compelling next direction for personal fabrication. From our experience with the prototypes we derive hardware requirements to make mobile fabrication also technically feasible.","",6,1476670800,1476672000,540,"Author"],[117,6,"uist3223","Full","Crowdsourced Fabrication","In recent years, extensive research in the HCI literature has explored interactive techniques for digital fabrication. However, little attention in this body of work has examined how to involve and guide human workers in fabricating larger-scale structures. We propose a novel model of crowdsourced fabrication, in which a large number of workers and volunteers are guided through the process of building a pre-designed structure. The process is facilitated by an intelligent construction space capable of guiding individual workers and coordinating the overall build process. More specifically, we explore the use of smartwatches, indoor location sensing, and instrumented construction materials to provide real-time guidance to workers, coordinated by a foreman engine that manages the overall build process. We report on a three day deployment of our system to construct a 12�-tall bamboo pavilion with assistance from more than one hundred volunteer workers, and reflect on observations and feedback collected during the exhibit. ","",6,1476672000,1476673200,540,"Author"],[118,6,"uist1001","Full","Reprise: A Design Tool for Specifying, Generating, and Customizing 3D Printable Adaptations on Everyday Objects","Everyday tools and objects often need to be customized for an unplanned use or adapted for specific user, such as adding a bigger pull to a zipper or a larger grip for a pen. The advent of low-cost 3D printing offers the possibility to rapidly construct a wide range of such adaptations. However, while 3D printers are now affordable enough for even home use, the tools needed to design custom adaptations normally require skills that are beyond users with limited 3D modeling experience.  \\  \\ In this paper, we describe Reprise--a design tool for specifying, generating, customizing and fitting adaptations onto existing household objects. Reprise allows users to express at a high level what type of action is applied to an object.  Based on this high level specification, Reprise automatically generates adaptations. Users can use simple sliders to customize the adaptations to better suit their particular needs and preferences, such as increasing the tightness for gripping, enhancing torque for rotation, or making a larger base for stability. Finally, Reprise provides a toolkit of fastening methods and support structures for fitting the adaptations onto existing objects. \\  \\ To validate our approach, we used Reprise to replicate 15 existing adaptation examples, each of which represents a specific category in a design space distilled from an analysis of over 3000 cases found in the literature and online communities. We believe this work would benefit makers and designers for prototyping lifehacking solutions and assistive technologies.","",6,1476673200,1476674400,540,"Author"],[119,6,"uist1189","Short","Exploring the Design Space for Energy-Harvesting Situated Displays","We explore the design space of energy-neutral situated displays, which give physical presence to digital information. We investigate three central dimensions: energy sources, display technologies, and wireless communications. Based on the power implications from our analysis, we present a thin, wireless, photovoltaic-powered display that is quick and easy to deploy and capable of indefinite operation in indoor lighting conditions. The display uses a low-resolution e-paper architecture, which is 35 times more energy-efficient than smaller-sized high-resolution displays. We present a detailed analysis on power consumption, photovoltaic energy harvesting performance, and a detailed comparison to other display-driving architectures. Depending on the ambient lighting, the display can trigger an update every 1 - 25 minutes and communicate to a PC or smartphone via Bluetooth Low-Energy.","",6,1476674400,1476675000,540,"Author"],[120,7,"uist4470","Full","FaceTouch: Enabling Touch Interaction in Display Fixed UIs for Mobile Virtual Reality","We present FaceTouch, a novel interaction concept for mobile Virtual Reality (VR) head-mounted displays (HMDs) that leverages the backside as a touch-sensitive surface. With FaceTouch, the user can point at and select virtual content inside their field-of-view by touching the corresponding location at the backside of the HMD utilizing their sense of proprioception. This allows for rich interaction (e.g. gestures) in mobile and nomadic scenarios without having to carry additional accessories (e.g. a gamepad). We built a prototype of FaceTouch and conducted two user studies. In the first study we measured the precision of FaceTouch in a display-fixed target selection task using three different selection techniques showing a low error rate of 2% indicate the viability for everyday usage. To asses the impact of different mounting positions on the user performance we conducted a second study. We compared three mounting positions of the touchpad (face, hand and side) showing that mounting the touchpad at the back of the HMD resulted in a significantly lower error rate, lower selection time and higher usability. Finally, we present interaction techniques and three example applications that explore the FaceTouch design space.","",7,1476670800,1476672000,540,"Author"],[121,7,"uist2538","Full","Supporting Mobile Sensemaking Through Intentionally Uncertain Highlighting","Patients researching medical diagnoses, scientist exploring new fields of literature, and students learning about new domains are all faced with the challenge of capturing information they find for later use. However, saving information is challenging on mobile devices, where the small screen and font sizes combined with the inaccuracy of finger based touch screens makes it time consuming and stressful for people to select and save text for future use. Furthermore, beyond the challenge of simply selecting a region of bounded text on a mobile device, in many learning and data exploration tasks the boundaries of what text may be relevant and useful later are themselves uncertain for the user. In contrast to previous approaches which focused on speeding up the selection process by making the identification of hard boundaries faster, we introduce the idea of intentionally supporting uncertain input in the context of saving information during complex reading and information exploration. We embody this idea in a system that uses force touch and fuzzy bounding boxes along with posthoc expandable context to support identifying and saving information in an intentionally uncertain way on mobile devices. In a two part user study we find that this approach reduced selection time and was preferred by participants over the default system text selection method.","",7,1476672000,1476673200,540,"Author"],[122,7,"uist2051","Full","HoloFlex: A Flexible Light-Field Smartphone with a Microlens Array and a P-OLED Touchscreen","We present HoloFlex, a 3D flexible smartphone featuring a light-field display consisting of a high-resolution P-OLED display and an array of 16,640 microlenses. HoloFlex allows mobile users to interact with 3D images featuring natural visual cues such as motion parallax and stereoscopy without glasses or head tracking. Its flexibility allows the use of bend input for interacting with 3D objects along the z axis. Images are rendered into 12-pixel wide circular blocks—pinhole views of the 3D scene—which enable ~80 unique viewports at an effective resolution of 160 � 104. The microlens array distributes each pixel from the display in a direction that preserves the angular information of light rays in the 3D scene. We present a preliminary study evaluating the effect of bend input vs. a vertical touch screen slider on 3D docking performance. Results indicate that bend input significantly improves movement time in this task. We also present 3D applications including a 3D editor, a 3D Angry Birds game and a 3D teleconferencing system that utilize bend input.","",7,1476673200,1476674400,540,"Author"],[123,7,"uist4063","Full","AuraSense: Enabling Expressive Around-Smartwatch Interactions with Electric Field Sensing","Existing smartwatches rely on touchscreens for display and input, which inevitably leads to finger occlusion and confines interactivity to a small area. In this work, we introduce AuraSense, which enables rich, around-device, smartwatch interactions using electric field sensing as an adapted device. To explore how this sensing approach could enhance smartwatch interactions, we considered different antenna configurations and how they could enable useful interaction modalities. We identified four configurations that can support six well-known modalities of particular interest and utility, including gestures above or in close proximity to watches, and touchscreen-like finger tracking on the skin. We quantify the feasibility of these input modalities, suggesting that AuraSense can be low latency and robust across users and environments.","",7,1476674400,1476675600,540,"Author"],[124,9,"uist4806","Full","ChainFORM: A Linear Integrated Modular Hardware System for Shape Changing Interfaces","This paper presents ChainFORM: a linear, modular, actuated hardware system as a novel type of shape changing interface. Using rich sensing and actuation capability, this modular hardware system allows users to construct and customize a wide range of interactive applications. Inspired by modular and serpentine robotics, our prototype comprises identical modules that connect in a chain. Modules are equipped with rich input and output capability: touch detection on multiple surfaces, angular detection, visual output, and motor actuation. Each module includes a servo motor wrapped with a flexible circuit board with an embedded microcontroller. \\  \\ Leveraging the modular functionality, we introduce novel interaction capability with shape changing interfaces, such as rearranging the shape/configuration and attaching to passive objects and bodies. To demonstrate the capability and interaction design space of ChainFORM, we implemented a variety of applications for both computer interfaces and hands-on prototyping tools.","",6,1476680400,1476681600,540,"Author"],[125,9,"uist2890","Full","Zooids: Building Blocks for Swarm User Interfaces","This paper introduces swarm user interfaces, a new class of human-computer interfaces comprised of many autonomous robots that handle both display and interaction. We describe the design of Zooids, an open-source open-hardware platform for developing tabletop swarm interfaces. The platform consists of a collection of custom-designed wheeled micro robots each 2.6 cm in diameter, a radio base-station, a high-speed DLP structured light projector for optical tracking, and a software framework for application development and control.  \\ We illustrate the potential of tabletop swarm user interfaces through a set of application scenarios developed with Zooids, and discuss general design considerations unique to swarm user interfaces.","",6,1476681600,1476682800,540,"Author"],[126,9,"uist2387","Full","Rovables: Miniature On-Body Robots as Mobile Wearables","We introduce Rovables, a miniature robot that can move freely on unmodified clothing. The robots are held in place by magnetic wheels, and can climb vertically. The robots are untethered and have an onboard battery, microcontroller, and wireless communications. They also contain a low-power localization system that uses wheel encoders and IMU, allowing Rovables to perform limited autonomous navigation on the body. In the technical evaluations, we found that Rovables can operate continuously for 45 minutes and can carry up to 1.5N. We propose an interaction space for mobile on-body devices spanning sensing, actuation, and interfaces, and develop application scenarios in that space. Our applications include on-body sensing, modular displays, tactile feedback and interactive clothing and jewelry. ","",6,1476682800,1476684000,540,"Author"],[127,9,"uist1731","Full","aeroMorph - Heat-sealing Inflatable Shape-change Materials for Interaction Design","This paper presents a design, simulation, and fabrication pipeline for making transforming inflatables with various materials.  \\ We introduce a bending mechanism that creates multiple, programmable shape-changing behaviors with inextensible materials, including paper, plastics and fabrics. \\ We developed a software tool that generates these bending mechanism for a given geometry, simulates its transformation, and exports the compound geometry as digital fabrication files. \\ We show a range of fabrication methods, from manual sealing, to heat pressing with custom stencils and a custom heat-sealing head that can be mounted on usual 3-axis CNC machines to precisely fabricate the designed transforming material. \\ Finally, we present three applications to show how this technology could be used for designing interactive wearables, toys, and furniture.","",6,1476684000,1476685200,540,"Author"],[128,10,"uist4523","Full","Beyond Snapping: Persistent, Tweakable Alignment and Distribution with StickyLines","Aligning and distributing graphical objects is a common, but cumbersome task. In a preliminary study (six graphic designers, six non-designers), we identified three key problems with current tools: lack of persistence, unpredictability of results, and inability to 'tweak' the layout. We created StickyLines, a tool that treats guidelines as first-class objects: Users can create precise, predictable and persistent interactive alignment and distribution relationships, and 'tweaked' positions can be maintained for subsequent interactions. We ran a [2x2] within-participant experiment to compare StickyLines with standard commands, with two levels of layout difficulty. StickyLines performed 40% faster and required 49% fewer actions than traditional alignment and distribution commands for complex layouts. In study three, six professional designers quickly adopted StickyLines and identified novel uses, including creating complex compound guidelines and using them for both spatial and semantic grouping.","",7,1476680400,1476681600,540,"Author"],[129,10,"uist3356","Full","Porous Interfaces for Small Screen Multitasking using Finger Identification","The lack of dedicated multitasking interface features in smartphones has resulted in users attempting a sequential form of multitasking via frequent app switching. In addition to the obvious temporal cost, it requires physical and cognitive effort which increases multifold as the back and forth switching becomes more frequent. We propose porous interfaces, a paradigm that combines the concept of translucent windows with finger identification to support efficient multitasking on small screens. Porous interfaces enable partially transparent app windows overlaid on top of each other, each of them being accessible simultaneously using a different finger as input. We design porous interfaces to include a broad range of multitasking interactions with and between windows, while ensuring fidelity with the existing smartphone interactions. We develop an end-to-end smartphone interface that demonstrates porous interfaces. In a qualitative study, participants found porous interfaces intuitive, easy, and useful for frequent multitasking scenarios.","",7,1476681600,1476682800,540,"Author"],[130,10,"uist2678","Full","Mining Controller Inputs to Understand Gameplay","Today's game analytics systems are powered by event logs, which reveal information about what players are doing but offer little insight about the types of gameplay that games foster. Moreover, the concept of gameplay itself is difficult to define and quantify. In this paper, we show that analyzing players' controller inputs using probabilistic topic models allows game developers to describe the types of gameplay � or action � in games in a quantitative way. More specifically, developers can discover the types of action that a game fosters and the extent that each game level fosters each type of action, all in an unsupervised manner. They can use this information to verify that their levels feature the appropriate style of gameplay and to recommend levels with gameplay that is similar to levels that players like. We begin with latent Dirichlet allocation (LDA), the simplest topic model, then develop the player–gameplay action (PGA) model to make the same types of discoveries about gameplay in a way that is independent of each player's play style. We train a player recognition system on the PGA model's output to verify that its discoveries about gameplay are in fact independent of each player's play style. The system recognizes players with over 90% accuracy in about 20 seconds of playtime.","",7,1476682800,1476684000,540,"Author"],[131,10,"uist2373","Full","TRing: Instant and Customizable Interactions with Objects Using an Embedded Magnet and a Finger-Worn Device","We present TRing, a finger-worn input device which provides instant and customizable interactions. TRing offers a novel method for making plain objects interactive using an embedded magnet and a finger-worn device. With a particle filter integrated magnetic sensing technique, we compute the fingertip’s position relative to the embedded magnet. We also offer a magnet placement algorithm that guides the magnet installation location based upon the user’s interface customization. By simply inserting or attaching a small magnet, we bring interactivity to both fabricated and existing objects. In our evaluations, TRing shows an average tracking error of 8.6 mm in 3D space and a 2D targeting error of 4.96 mm, which are sufficient for implementing average-sized conventional controls such as buttons and sliders. A user study validates the input performance with TRing on a targeting task (92% accuracy within 45 mm distance) and a cursor control task (91% accuracy for a 10 mm target). Furthermore, we show examples that highlight the interaction capability of our approach.","",7,1476684000,1476685200,540,"Author"],[132,12,"uist4646","Full","Designing a Non-contact Wearable Tactile Display Using Airflows","Traditional wearable tactile displays transfer tactile stimulations \\ through a firm contact between the stimulator and the \\ skin. We conjecture that a firm contact may not be always \\ possible and acceptable. Therefore, we explored the concept \\ of a non-contact wearable tactile display using an airflow, \\ which can transfer information without a firm contact. To secure \\ an empirical ground for the design of a wearable airflow \\ display, we conducted a series of psychophysical experiments \\ to estimate the intensity thresholds, duration thresholds, and \\ distance thresholds of airflow perception on various body locations, \\ and report the resulting empirical data in this paper. \\ We then built a 4-point airflow display, compared its performance \\ with that of a vibrotactile display, and could show \\ that the two tactile displays are comparable in information \\ transfer performance. User feedback was also positive and \\ revealed many unique expressions describing airflow-based \\ tactile experiences. Lastly, we demonstrate the feasibility of \\ an airflow-based wearable tactile display with a prototype using \\ micro-fans.","",6,1476687600,1476688800,540,"Author"],[133,12,"uist3095","Full","RealPen: Providing Realism in Handwriting Tasks on Touch Surfaces using Auditory-Tactile Feedback","We present RealPen, an augmented stylus for capacitive tablet screens that recreates the physical sensation of writing on paper with a pencil, ball-point pen or marker pen. The aim is to create a more engaging experience when writing on touch surfaces, such as screens of tablet computers. This is achieved by regenerating the friction-induced oscillation and sound of a real writing tool in contact with paper. To generate realistic tactile feedback, our algorithm analyzes the frequency spectrum of the friction oscillation generated when writing with traditional tools, extracts principal frequencies, and uses the actuator’s frequency response profile for an adjustment weighting function. We enhance the realism by providing the sound feedback aligned with the writing pressure and speed. Furthermore, we investigated the effects of superposition and fluctuation of several frequencies on human tactile perception, evaluated the performance of RealPen, and characterized users� perception and preference of each feedback type. ","",6,1476688800,1476690000,540,"Author"],[134,12,"uist2378","Full","Muscle-plotter: an Interactive System based on Electrical Muscle Stimulation that Produces Spatial Output","We explore how to create interactive systems based on electrical muscle stimulation that offer expressive output. We present muscle-plotter, a system that provides users with input and output access to a computer system while on the go. Using pen-on-paper interaction, muscle-plotter allows users to engage in cognitively demanding activities, such as writing math. Users write formulas using a pen and the system responds by making the users� hand draw charts and widgets. While Anoto technology in the pen tracks users� input, muscle-plotter uses electrical muscle stimulation (EMS) to steer the user’s wrist so as to plot charts, fit lines through data points, find data points of interest, or fill in forms. We demonstrate the system at the example of six simple applications, including a wind tunnel simulator. �  � The key idea behind muscle-plotter is to make the user’s hand sweep an area on which muscle-plotter renders curves, i.e., series of values, and to persist this EMS output by means of the pen. This allows the system to build up a larger whole. Still, the use of EMS allows muscle-plotter to achieve a compact and mobile form factor. In our user study, muscle-plotter made participants draw random plots with an accuracy of ±4.07 mm and preserved the frequency of functions to be drawn up to 0.3 cycles per cm.  � ","",6,1476690000,1476691200,540,"Author"],[135,12,"uist3488","Full","Haptic Learning of Semaphoric Finger Gestures","Haptic learning of gesture shortcuts has never been explored. In this paper, we investigate haptic learning of a freehand semaphoric finger tap gesture shortcut set using haptic rings. We conduct a two-day study of 30 participants where we couple haptic stimuli with visual and audio stimuli, and compare their learning performance with wholly visual learning. The results indicate that with <30 minutes of learning, haptic learning of finger tap semaphoric gestures is comparable to visual learning and maintains its recall on the second day.","",6,1476691200,1476692400,540,"Author"],[136,12,"uist2457","Full","GyroVR: Simulating Inertia in Virtual Reality using Head Worn Flywheels","We present GyroVR, head worn flywheels designed to render inertia in Virtual Reality (VR. Motions such as flying, diving or floating in outer space generate kinesthetic forces onto our body which impede movement and are currently not represented in VR. We simulate those kinesthetic forces by attaching flywheels to the users head, leveraging the gyroscopic effect of resistance when changing the spinning axis of rotation. GyroVR is an ungrounded, wireless and self contained device allowing the user to freely move inside the virtual environment. The generic shape allows to attach it to different positions on the users body. We evaluated the impact of GyroVR onto different mounting positions on the head (back and front) in terms of immersion, enjoyment and simulator sickness. Our results show, that attaching GyroVR onto the users head (front of the Head Mounted Display (HMD)) resulted in the highest level of immersion and enjoyment and therefore can be built into future VR HMDs, enabling kinesthetic forces in VR.","",6,1476692400,1476693600,540,"Author"],[137,13,"uist4158","Full","Telescope: Fine-Tuned Discovery of Interactive Web UI Feature Implementation","Professional websites contain rich interactive features that developers can learn from, yet understanding their implementation remains a challenge due to the nature of unfamiliar code. Existing tools provide affordances to analyze source code, but feature-rich websites reveal tens of thousands of lines of code and can easily overwhelm the user. We thus present Telescope, a platform for discovering how JavaScript and HTML support a website interaction. Telescope helps users understand unfamiliar website code through a composite view they control by adjusting JavaScript detail, scoping the runtime timeline, and triggering relational links between JS, HTML, and website components. To support these affordances on the open web, Telescope instruments the JavaScript in a website without request intercepts using a novel sleight-of-hand technique, then watches for traces emitted from the website. In a case study across seven popular websites, Telescope helped identify less than 150 lines of front-end code out of tens of thousands that accurately describe the desired interaction in six of the sites. In an exploratory user study, we observed users identifying difficult programming concepts by developing strategies to analyze relatively small amounts of unfamiliar website source code with Telescope. ","",7,1476687600,1476688800,540,"Author"],[138,13,"uist2681","Full","CodeMend: Assisting Interactive Programming with Bimodal Embedding","Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. However, the process of finding and integrating a working solution is often very time-consuming. Though code search engines have increased in quality, there remain significant language- and workflow-gaps in meeting end-user needs. Novice and intermediate programmers often lack the language to query, and the expertise in transferring found code to their task. To address this problem, we present CodeMend, a system to support finding and integration of code. CodeMend leverages a neural embedding model to jointly model natural language and code as mined from large Web and code datasets. We also demonstrate a novel, mixed-initiative, interface to support query and integration steps. Through CodeMend, end-users describe their goal in natural language. The system makes salient the relevant API functions, the lines in the end-user's program that should be changed, as well as proposing the actual change. We demonstrate the utility and accuracy of CodeMend through lab and simulation studies. ","",7,1476688800,1476690000,540,"Author"],[139,13,"uist2420","Full","Meta: Enabling Programming Languages to Learn from the Crowd","Collectively authored programming resources such as Q&A sites and open-source libraries provide a limited window into how programs are constructed, debugged, and run. To address these limitations, we introduce Meta: a language extension for Python that allows programmers to share functions and track how they are used by a crowd of other programmers. Meta functions are shareable via URL and instrumented to record runtime data. Combining thousands of Meta functions with their collective runtime data, we demonstrate tools including an optimizer that replaces your function with a more efficient version written by someone else, an auto-patcher that saves your program from crashing by finding equivalent functions in the community, and a proactive linter that warns you when a function fails elsewhere in the community. We find that professional programmers are able to use Meta for complex tasks (creating new Meta functions that, for example, cross-validate a logistic regression), and that Meta is able to find 44 optimizations (for a 1.45 times average speedup) and 5 bug fixes across the crowd.","",7,1476690000,1476691200,540,"Author"],[140,13,"uist1574","Full","Holoportation: Virtual 3D Teleportation in Real-time","We present an end-to-end system for augmented and virtual reality telepresence, called Holoportation. Our system demonstrates high-quality, real-time 3D reconstructions of an entire space, including people, furniture and objects, using a set of new depth cameras. These 3D models can also be transmitted in real-time to remote users. This allows users wearing virtual or augmented reality displays to see, hear and interact with remote participants in 3D, almost as if they were present in the same physical space. From an audio-visual perspective, communicating and interacting with remote users edges closer to face-to-face communication. This paper describes the Holoportation technical system in full, its key interactive capabilities, the application scenarios it enables, and an initial qualitative study of using this new communication medium.","",7,1476691200,1476692400,540,"Author"],[141,16,"uist4939","Full","Next-Point Prediction Metrics for Perceived Spatial Errors","Touch screens have a delay between user input and corresponding visual interface feedback, called input ``latency'' (or ``lag''). Visual latency is more noticeable during continuous input actions like dragging, so methods to display feedback based on the most likely path for the next few input points have been described in research papers and patents. Designing these ``next-point prediction'' methods is challenging, and there have been no standard metrics to compare different approaches. We introduce metrics to quantify the probability of 7 spatial error ``side-effects'' caused by next-point prediction methods. Types of side-effects are derived using a thematic analysis of comments gathered in a 12 participants study covering drawing, dragging, and panning tasks using 5 state-of-the-art next-point predictors. Using experiment logs of actual and predicted input points, we develop quantitative metrics that correlate positively with the frequency of perceived side-effects. These metrics enable practitioners to compare next-point predictors using only input logs. ","",6,1476750600,1476751800,540,"Author"],[142,16,"uist3714","Full","Wearables as Context for Guiard-abiding Bimanual Touch","We explore the contextual details afforded by wearable devices to support multi-user, direct-touch interaction on electronic whiteboards in a way that—unlike previous work—can be fully consistent with natural bimanual-asymmetric interaction as set forth by Guiard. �   � Our work offers the following key observation. While Guiard’s framework has been widely applied in HCI, for bimanual interfaces where each hand interacts via direct touch, subtle limitations of multi-touch technologies—as well as limitations in conception and design—mean that the resulting interfaces often cannot fully adhere to Guiard’s principles even if they want to. The interactions are fundamentally ambiguous because the system does not know which hand, left or right, contributes each touch. But by integrating additional context from wearable devices, our system can identify which user is touching, as well as distinguish what hand they use to do so. This enables our prototypes to respect lateral preference—the assignment of natural roles to each hand as advocated by Guiard—in a way that has not been articulated before.","",6,1476751800,1476753000,540,"Author"],[143,16,"uist1306","Full","Gaze and Touch Interaction on Tablets","We explore how gaze can support touch interaction on tablets. When holding the device, the free thumb is normally limited in reach, but can provide an opportunity for indirect touch input. Here we propose gaze and touch input, where touches redirect to the gaze target. This provides whole-screen reachability while only using a single hand for both holding and input. We present a user study comparing this technique to direct-touch, showing that users are slightly slower but can utilise one-handed use with less physical effort. To enable interaction with small targets, we introduce CursorShift, a method that uses gaze to provide users temporal control over cursors during direct-touch interactions. Taken together, users can employ three techniques on tablets: direct-touch, gaze and touch, and cursor input. In three applications, we explore how these techniques can coexist in the same UI and demonstrate how tablet tasks can be performed with thumb-only input of the holding hand, and with it describe novel interaction techniques for gaze based tablet interaction.","",6,1476753000,1476754200,540,"Author"],[144,16,"uist2777","Full","Predicting Finger-Touch Accuracy Based on the Dual Gaussian Distribution Model","Accurately predicting the accuracy of finger-touch target acquisition is crucial for designing touchscreen UI and for modeling complex and higher level touch interaction behaviors. Despite its importance, there has been little theoretical work on creating such models. Building on the Dual Gaussian Distribution Model[3], we derived an accuracy model that predicts the success rate of target acquisition based on the target size. We evaluated the model by comparing the predicted success rates with empirical measures for three types of targets including 1-dimensional vertical and horizontal, and 2-dimensional circular targets. The predictions matched the empirical data very well: the differences between predicted and observed success rates were under 5% for 4.8 mm and 7.2 mm targets, and under 10% for 2.4 mm targets. The evaluation results suggest that our simple model can reliably predict touch accuracy. ","",6,1476754200,1476755400,540,"Author"],[145,17,"uist4645","Full","ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers","Smartwatches and wearables are unique in that they reside on the body, presenting great potential for always-available input and interaction. Their position on the wrist makes them ideal for capturing bio-acoustic signals. We developed a custom smartwatch kernel that boosts the sampling rate of a smartwatch’s existing accelerometer to 4 kHz. Using this new source of high-fidelity data, we uncover a wide range of applications. For example, we use bio-acoustic data to classify hand gestures such as flicks, claps, scratches, and taps, which combine with on-device motion tracking to create a wide range of expressive input modalities. Bio-acoustic sensing can also detect the vibrations of grasped mechanical or motor-powered objects, enabling passive object recognition that can augment everyday experiences with context-sensitive functionality. Finally, we can generate structured vibrations using a transducer, and show that arbitrary data can be transmitted through the body. Overall, our contributions unlock user interface techniques that previously relied on special-purpose and/or cumbersome instrumentation, making such interactions considerably more feasible for inclusion in future consumer devices.","",7,1476750600,1476751800,540,"Author"],[146,17,"uist4226","Full","proCover: Sensory Augmentation of Prosthetic Limbs Using Smart Textile Covers","Today’s commercially available prosthetic limbs lack tactile sensation and feedback.  Recent research in this domain focuses on sensor technologies designed to be directly embedded into future prostheses. We present a novel concept and prototype of a prosthetic-sensing wearable that offers a non-invasive, self-applicable and customizable approach for the sensory augmentation of present-day and future low to mid-range priced lower-limb prosthetics. From consultation with eight lower-limb amputees, we investigated the design space for prosthetic sensing wearables and developed novel interaction methods for dynamic, user-driven creation and mapping of sensing regions on the foot to wearable haptic feedback actuators. Based on a pilot-study with amputees, we assessed the utility of our design in scenarios brought up by the amputees and we summarize our findings to establish future directions for research into using smart textiles for the sensory enhancement of prosthetic limbs.","",7,1476751800,1476753000,540,"Author"],[147,17,"uist2454","Full","SleepCoacher: A Personalized Automated Self-Experimentation System for Sleep Recommendations","We present SleepCoacher, an integrated system implementing a framework for effective self-experiments. SleepCoacher automates the cycle of single-case experiments by collecting raw mobile sensor data and generating personalized, data-driven sleep recommendations based on a collection of template recommendations created with input from clinicians. The system guides users through iterative short experiments to test the effect of recommendations on their sleep. We evaluate SleepCoacher in two studies, measuring the effect of recommendations on the frequency of awakenings, self-reported restfulness, and sleep onset latency, concluding that it is effective: participant sleep improves as adherence with SleepCoacher's recommendations and experiment schedule increases. This approach presents computationally-enhanced interventions leveraging the capacity of a closed feedback loop system, offering a method for scaling guided single-case experiments in real time.","",7,1476753000,1476754200,540,"Author"],[148,17,"uist2626","Full","Bootstrapping User-Defined Body Tapping Recognition with Offline-Learned Probabilistic Representation","To address the increasing functionality (or information) overload of smartphones, prior research has explored a variety of methods to extend the input vocabulary of mobile devices. In particular, body tapping has been previously proposed as a technique that allows the user to quickly access a target functionality by simply tapping at a specific location of the body with a smartphone. Though compelling, prior work often fell short in enabling users� unconstrained tapping locations or behaviors. To address this problem, we developed a novel recognition method that combines both offline—before the system sees any user-defined gestures—and online learning to reliably recognize arbitrary, user-defined body tapping gestures, only using a smartphone’s built-in sensors. Our experiment indicates that our method significantly outperforms baseline approaches in several usage conditions. In particular, provided only with a single sample per location, our accuracy is 30.8% over an SVM baseline and 24.8% over a template matching method. Based on these findings, we discuss how our approach can be generalized to other user-defined gesture problems.","",7,1476754200,1476755400,540,"Author"],[149,19,"uist4832","Full","Eviza: A Natural Language Interface for Visual Analysis","Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Many of these systems have fundamental limitations. Most return minimally interactive visualizations in response to queries and often require experts to perform modeling for a set of predicted user queries before the systems are effective. Eviza provides a natural language interface for an interactive query \\\\textit{dialog} with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. The system employs a probabilistic grammar based approach with predefined rules that are dynamically updated based on the data from the visualization, as opposed to computationally intensive deep learning or knowledge based approaches. \\  \\ The result of an interaction is a change to the view (\\\\textit{e.g.,} filtering, navigation, selection) providing graphical answers and ambiguity widgets to handle ambiguous queries and system defaults. There is also rich domain awareness of time, space, and quantitative reasoning built in, and linking into existing knowledge bases for additional semantics. Eviza also supports pragmatics and exploring multi-modal interactions to help enhance the expressiveness of how users can ask questions about their data during the flow of visual analysis.","",6,1476757200,1476758400,540,"Author"],[150,19,"uist4362","Full","Semi-Automated SVG Programming via Direct Manipulation","Direct manipulation interfaces provide intuitive and interactive features to a broad range of users, but they often exhibit two limitations: the built-in features cannot possibly cover all use cases, and the internal representation of the content is not readily exposed. We believe that if direct manipulation interfaces were to (a) use general-purpose programs as the representation format, and (b) expose those programs to the user, then experts could customize these systems in powerful new ways and non-experts could enjoy some of the benefits of programmable systems. \\  \\ In recent work, we presented a prototype SVG editor called Sketch-n-Sketch that offered a step towards this vision. In that system, the user wrote a program in a general-purpose lambda-calculus to generate a graphic design and could then directly manipulate the output to indirectly change design parameters (i.e. constant literals) in the program in real-time during the manipulation. Unfortunately, the burden of programming the desired relationships rested entirely on the user. \\  \\ In this paper, we design and implement new features for Sketch-n-Sketch that assist in the programming process itself. Like typical direct manipulation systems, our extended Sketch-n-Sketch now provides GUI-based tools for drawing shapes, relating shapes to each other, and grouping shapes together. Unlike typical systems, however, each tool carries out the user's intention by transforming their general-purpose program. This novel, semi-automated programming workflow allows the user to rapidly create high-level, reusable abstractions in the program while at the same time retaining direct manipulation capabilities. In future work, our approach may be extended with more graphic design features or realized for other application domains.","",6,1476758400,1476759600,540,"Author"],[151,19,"uist3225","Full","Reading and Learning Smartfonts","As small displays on devices like smartwatches become increasingly common, many people have difficulty reading the text on these displays. Vision conditions like presbyopia that result in blurry near vision make reading small text particularly hard. We design multiple different scripts for displaying English text, legible at small sizes even when blurry, for small screens such as smartphones and smartwatches. These “smartfonts� redesign visual character presentations to improve the reading experience. Like cursive, Grade 1 Braille, and ordinary fonts, they preserve orthography and spelling. They have the potential to enable people to read more text comfortably on small screens, e.g., without reading glasses. To simulate presbyopia, we blur images and evaluate their legibility using paid crowdsourcing. We also evaluate the difficulty of learning to read smartfonts and observe a learnability/legibility trade-off. Our most learnable smartfont can be read at roughly half the speed of Latin after two thousand practice sentences. It is also legible smaller than half the size of traditional Latin (i.e. “English�) when blurry.","",6,1476759600,1476760800,540,"Author"],[152,19,"uist2534","Full","Interactive Volume Segmentation with Threshold Field Painting","An interactive method for segmentation and isosurface extraction of medical volume data is proposed. In conventional methods, users decompose a volume into multiple regions iteratively, segment each region using a threshold, and then manually clean the segmentation result by removing clutter in each region. However, this is tedious and requires many mouse operations from different camera views. We propose an alternative approach whereby the user simply applies painting operations to the volume using tools commonly seen in painting systems, such as flood fill and brushes. This significantly reduces the number of mouse and camera control operations. Our technical contribution is in the introduction of the threshold field, which assigns spatially-varying threshold values to individual voxels. This generalizes discrete decomposition of a volume into regions and segmentation using a constant threshold in each region, thereby offering a much more flexible and efficient workflow. This paper describes the details of the user interaction and its implementation. Furthermore, the results of a user study are discussed. The results indicate that the proposed method can be a few times faster than a conventional method.","",6,1476760800,1476762000,540,"Author"],[153,20,"uist4904","Full","SkyAnchor: Optical Design for Anchoring Mid-air Images onto Physical Objects","For glass-free mixed reality (MR), mid-air imaging is a promising way of superimposing a virtual image onto a real object. We focus on attaching virtual images to non-static real life objects. In previous work, moving the real object causes latency in the superimposing system, and the virtual  image seems to follow the object with a delay. This is caused by delays due to sensors, displays and computational devices for position sensing, and occasionally actuators for moving the image generation source. In order to avoid this problem, this paper proposes to separate the object-anchored imaging effect from the position sensing. Our proposal is a retro-reflective system called “SkyAnchor,’� which consists of only optical devices: two mirrors and an aerial-imaging plate. The system reflects light from a light source anchored under the physical object itself, and forms an image anchored around the object. This optical solution does not cause any latency in principle and is effective for high-quality mixed reality applications. We consider two types of light sources to be attached to physical objects: reflecting content from a touch table on which the object rests, or attaching the source directly on the object. As for position sensing, we utilize a capacitive marker on the bottom of the object, tracked on a touch table. We have implemented a prototype, where mid-air images move with the object, and whose content may change based on its position.","",7,1476757200,1476758400,540,"Author"],[154,20,"uist3354","Full","Changing the Appearance of Physical Interfaces Through Controlled Transparency","We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.","",7,1476758400,1476759600,540,"Author"],[155,20,"uist3084","Full","JOLED: A mid-air display based on electrostatic rotation of levitated Janus objects","We present JOLED, a mid-air display for interactive physical visualization using Janus objects as physical voxels. The Janus objects have special surfaces that have two or more asymmetric physical properties at different areas. In JOLED, they are levitated in mid-air and controllably rotated to reveal their different physical properties. We made voxels by coating the hemispheres of expanded polystyrene beads with different materials, and applied a thin patch of titanium dioxide to induce electrostatic charge on them. Transparent indium tin oxide electrodes are used around the levitation volume to create a tailored electric field to control the orientation of the voxels. We propose a novel method to control the angular position of individual voxels in a grid using electrostatic rotation and their 3D position using acoustic levitation. We present a display in which voxels can be flipped independently, and two mid-air physical games with a voxel as the playable character that moves in 3D across other physical structures and rotates to reflect its status in the games. We demonstrate a voxel update speed of 37.8 ms/flip, which is video-rate.","",7,1476759600,1476760800,540,"Author"],[156,20,"uist3589","Full","LIME: LIquid MEtal Interfaces for Non-Rigid Interaction","Room-temperature liquid metal GaIn25 (Eutectic Gallium- Indium alloy, 75% gallium and 25% indium) has distinctive properties of reversible deformation and controllable locomotion under an external electric field stimulus. Liquid metal’s newly discovered properties imply great possibilities in developing new technique for interface design. In this paper, we present LIME, LIquid MEtal interfaces for non-rigid interaction. We first discuss the interaction potential of LIME interfaces. Then we introduce the development of LIME cells and the design of some LIME widgets.","",7,1476760800,1476762000,540,"Author"],[157,20,"uist2948","Full","Phyxel: Realistic Display of Shape and Appearance using Physical Objects with High-speed Pixelated Lighting","A computer display that is sufficiently realistic such that the difference between a presented image and a real object cannot be discerned is in high demand in a wide range of fields, such as entertainment, digital signage, and design industry. \\ To achieve such a level of reality, it is essential to reproduce the three-dimensional (3D) shape and material appearances simultaneously; however, to date, developing a display that can satisfy both conditions has been difficult. \\ To address this problem, we propose a system that places physical elements at desired locations to create a visual image that is perceivable by the naked eye. This configuration can be realized by exploiting characteristics of human visual perception. \\ Humans perceive light modulation as perfectly steady light if the modulation rate is sufficiently high.  \\ Therefore, if high-speed spatially varying illumination is projected to the actuated physical elements possessing various appearances at the desired timing, a realistic visual image that can be transformed dynamically by simply modifying the lighting pattern can be obtained.  \\ We call the proposed display technology Phyxel.  \\ This paper describes the proposed configuration and required performance for Phyxel.  \\ We also demonstrate three applications: dynamic stop motion, a layered 3D display, and shape mixture. ","",7,1476762000,1476763200,540,"Author"],[158,22,"uist4594","Full","Private Webmail 2.0: Simple and Easy-to-Use Secure Email","Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.","",6,1476766800,1476768000,540,"Author"],[159,22,"uist4163","Full","CloakingNote: A Novel Desktop Interface for Subtle Writing Using Decoy Texts","We present CloakingNote, a novel desktop interface for subtle writing. The main idea of CloakingNote is to misdirect observers� attention away from a real text by using a prominent decoy text. To assess the subtlety of CloakingNote, we conducted a subtlety test while varying the contrast ratio between the real text and its background. Our results demonstrated that the real text as well as the interface itself were subtle even when participants were aware that a writer might be engaged in suspicious activities. We also evaluated the feasibility of CloakingNote through a performance test and categorized the users� layout strategies.","",6,1476768000,1476769200,540,"Author"],[160,22,"uist3160","Full","Mavo: Creating Interactive Data-Driven Web Applications by Authoring HTML","Many people can author static web pages with HTML and CSS but find it hard or impossible to program persistent, interactive web \\ applications. We show that for a broad class of CRUD (Create, Read, Update, Delete) applications, this gap can be bridged.  \\ Mavo extends the declarative syntax of HTML to describe Web applications that manage, store and transform data.  \\ Using Mavo, authors with basic HTML knowledge define complex data schemas implicitly as they design their HTML layout. They need \\ only add a few attributes and expressions to their HTML elements to transform their static  \\ design into a persistent, data-driven web application whose data can be edited by direct manipulation of the content in the browser. We evaluated Mavo with 20 users who marked up static designs---some provided by us, some their own \\ creation---to transform them into fully functional web applications.  Even users with no programming experience \\ were able to quickly craft Mavo applications.","",6,1476769200,1476770400,540,"Author"],[161,23,"uist4096","Full","QuickCut: An Interactive Tool for Editing Narrated Video","We present QuickCut, an interactive video editing tool designed to help authors efficiently edit narrated video. QuickCut takes an audio recording of the narration voiceover and a collection of raw video footage as input.  Users then review the raw footage and provide spoken annotations describing the relevant actions and objects in the scene. QuickCut time-aligns a transcript of the annotations with the raw footage and a transcript of the narration to the voiceover. These aligned transcripts enable authors to quickly match story events in the narration with semantically relevant video segments and form alignment constraints between them. \\ Given a set of such constraints, QuickCut applies dynamic programming optimization to choose frame-level cut points between the video segments while maintaining alignments with the narration and adhering to low-level film editing guidelines.  We demonstrate QuickCut's effectiveness by using it to generate a variety of short (less than 2 minutes) narrated videos. \\ Each result required between 14 and 52 minutes of user time to edit (i.e. between 8 and 31 minutes for each minute of output video), which is far less than typical authoring times with existing video editing workflows.  ","",7,1476766800,1476768000,540,"Author"],[162,23,"uist3553","Full","Dynamic Authoring of Audio with Linked Scripts","Speech recordings are central to modern media from podcasts to  audio books to e-lectures and voice-overs. Authoring these recordings involves an iterative back and forth process between script writing/editing and audio recording/editing. Yet, most existing tools treat the script and the audio separately, making the back and forth workflow very tedious. We present Voice Script, an interface to support a dynamic workflow for script writing and  audio recording/editing. Our system integrates the script with the audio such that, as the user writes the script or records speech, edits to the script are translated to the audio and vice versa. Through informal user studies, we  demonstrate that our interface greatly facilitates the audio authoring process in various scenarios.  ","",7,1476768000,1476769200,540,"Author"],[163,23,"uist3220","Full","VidCrit: Video-based Asynchronous Video Review","Video production is a collaborative process in which stakeholders regularly review drafts of the edited video to indicate problems and offer suggestions for improvement. Although practitioners prefer in-person feedback, most reviews are conducted asynchronously via email due to scheduling and location constraints. The use of this impoverished medium is challenging for both providers and consumers of feedback. We introduce VidCrit, a system for providing asynchronous feedback on drafts of edited video that incorporates favorable qualities of an in-person review. This system consists of two separate interfaces: (1) A feedback recording interface captures reviewers' spoken comments, mouse interactions, hand gestures and other physical reactions. (2) A feedback viewing interface transcribes and segments the recorded review into topical comments so that the video author can browse the review by either text or timelines.     \\ Our system features novel methods to automatically segment a long review session into topical text comments, and to label such comments with additional contextual information. We interviewed practitioners to inform a set of design guidelines for giving and receiving feedback, and based our system's design on these guidelines. \\ Video reviewers using our system preferred our feedback recording interface over email for providing feedback due to the reduction in time and effort.  \\ In a fixed amount of time, reviewers provided 10.9 (�=5.09) more local comments than when using text. � All video authors rated our feedback viewing interface preferable to receiving feedback via e-mail.","",7,1476769200,1476770400,540,"Author"],[164,25,"uist2588","Full","Metamaterial Mechanisms","Recently, researchers started to engineer not only the outer shape of objects, but also their internal microstructure. Such objects, typically based on 3D cell grids, are also known as metamaterials. Metamaterials have been used, for example, to create materials with soft and hard regions. \\  \\ So far, metamaterials were understood as materials—we want to think of them as machines. We demonstrate metamaterial objects that perform a mechanical function. Such metamaterial mechanisms consist of a single block of material the cells of which play together in a well-defined way in order to achieve macroscopic movement. Our metamaterial door latch, for example, transforms the rotary movement of its handle into a linear motion of the latch. Our metamaterial Jansen walker consists of a single block of cells—that can walk. The key element behind our metamaterial mechanisms is a specialized type of cell, the only ability of which is to shear.  �  � In order to allow users to create metamaterial mechanisms efficiently we implemented a specialized 3D editor. It allows users to place different types of cells, including the shear cell, thereby allowing users to add mechanical functionality to their objects. To help users verify their designs during editing, our editor allows users to apply forces and simulates how the object deforms in response. ","",6,1476772800,1476774000,540,"Author"],[165,25,"uist2352","Full","Digital Gastronomy: Methods & Recipes for Hybrid Cooking","","",6,1476774000,1476775200,540,"Author"],[166,25,"uist2014","Full","A 3D Printer for Interactive Electromagnetic Devices","We introduce a new form of low-cost 3D printer to print interactive electromechanical objects with wound in place coils. At the heart of this printer is a mechanism for depositing wire within a five degree of freedom (5DOF) fused deposition modeling (FDM) 3D printer. Copper wire can be used with this mechanism to form coils which induce magnetic fields as a current is passed through them.  Soft iron wire can additionally be used to form components with high magnetic permeability which are thus able to shape and direct these magnetic fields to where they are needed. When fabricated with structural plastic elements, this allows simple but complete custom electromagnetic devices to be 3D printed. As examples, we demonstrate the fabrication of a solenoid actuator for the arm of a Lucky Cat figurine, a 6-pole motor stepper stator, a reluctance motor rotor and a Ferrofluid display. In addition, we show how printed coils which generate small currents in response to user actions can be used as input sensors in interactive devices. ","",6,1476775200,1476776400,540,"Author"],[167,25,"uist1432","Full","Foundry: Hierarchical Material Design for Multi-Material Fabrication","We demonstrate a new approach for designing functional material definitions for multi-material fabrication using our system called Foundry. Foundry provides an interactive and visual process for hierarchically designing spatially-varying material properties (e.g., appearance, mechanical, optical). The resulting meta-materials exhibit structure at the micro and macro level and can surpass the qualities of traditional composites. The material definitions are created by composing a set of operators into an operator graph. Each operator performs a volume decomposition operation, remaps space, or constructs and assigns a material composition. The operators are implemented using a domain-specific language for multi-material fabrication; users can easily extend the library by writing their own operators. Foundry can be used to build operator graphs that describe complex, parameterized, resolution-independent, and reusable material definitions. We also describe how to stage the evaluation of the final material definition which in conjunction with progressive refinement, allows for interactive material evaluation even for complex designs. We show sophisticated and functional parts designed with our system. \\ ","",6,1476776400,1476777600,540,"Author"],[168,26,"uist4946","Full","DriftBoard: A Panning-Based Text Entry Technique for Ultra-Small Touchscreens","Emerging ultra-small wearables like smartwatches pose a design challenge for touch-based text entry. This is due to the \"fat-finger problem,\" wherein users struggle to select elements much smaller than their fingers. To address this challenge, we developed DriftBoard, a panning-based text entry technique where the user types by positioning a movable qwerty keyboard on an interactive area with respect to a fixed cursor point. In this paper, we describe the design and implementation of DriftBoard and report results of a user study on a watch-size touchscreen. The study compared DriftBoard to two ultra-small keyboards, ZoomBoard (tapping-based) and Swipeboard (swiping-based). DriftBoard performed comparably (no significant difference) to ZoomBoard in the major metrics of text entry speed and error rate, and outperformed Swipeboard, which suggests that panning-based typing is a promising input method for text entry on ultra-small touchscreens.","",7,1476772800,1476774000,540,"Author"],[169,26,"uist3526","Full","Expressive Keyboards: Enriching Gesture-Typing on Mobile Devices","Gesture-typing is an efficient, easy-to-learn, and errortolerant technique for entering text on software keyboards. Our goal is to ��ecycle� users� otherwise-unused gesture variation to create rich output under the users� control, without sacrificing accuracy. Experiment 1 reveals a high level of existing gesture variation, even for accurate text, and shows that users can consciously vary their gestures under different conditions. We designed an Expressive Keyboard for a smart \\ phone which maps input gesture features identified in Experiment 1 to a continuous output parameter space, i.e. RGB color. Experiment 2 shows that users can consciously modify \\ their gestures, while retaining accuracy, to generate specific colors as they gesture-type. Users are more successful when they focus on output characteristics (such as red) rather than input characteristics (such as curviness). We designed an app with a dynamic font engine that continuously interpolates between several typefaces, as well as controlling weight and random variation. Experiment 3 shows that, in the context of a more ecologically-valid conversation task, users enjoy generating multiple forms of rich output. We conclude with suggestions for how the Expressive Keyboard approach can enhance a wide variety of gesture recognition applications.","",7,1476774000,1476775200,540,"Author"],[170,26,"uist1914","Full","EdgeVib: Effective Alphanumeric Character Output Using a Wrist-Worn Tactile Display","This paper presents EdgeVib, a system of spatiotemporal vibration patterns for delivering alphanumeric characters on wrist-worn vibrotactile displays. We first investigated spatiotemporal pattern delivery through a watch-back tactile display by performing a series of user studies. The results reveal that employing a 2�2 vibrotactile array is more effective than employing a 3�3 one, because the lower-resolution array creates clearer tactile sensations in less time consumption. We then deployed EdgeWrite patterns on a 2�2 vibrotactile array to determine any difficulties of delivering alphanumerical characters, and then modified the unistroke patterns into multistroke EdgeVib ones on the basis of the findings. The results of a 24-participant user study reveal that the recognition rates of the modified multistroke patterns were significantly higher than the original unistroke ones in both alphabet (85.9% vs. 70.7%) and digits (88.6% vs. 78.5%) delivery, and a further study indicated that the techniques can be generalized to deliver two-character compound messages with recognition rates higher than 83.3%. The guidelines derived from our study can be used for designing watch-back tactile displays for alphanumeric character output.","",7,1476775200,1476776400,540,"Author"],[171,26,"uist4718","Full","On Suggesting Phrases vs. Predicting Words for Mobile Text Composition","A system capable of suggesting multi-word phrases while someone is writing could supply ideas about content and phrasing and allow those ideas to be inserted efficiently. Meanwhile, statistical language modeling has provided various approaches to predicting phrases that users type. We introduce a simple extension to the familiar mobile keyboard suggestion interface that presents phrase suggestions that can be accepted by a repeated-tap gesture. In an extended composition task, we found that phrases were interpreted as suggestions that affected the content of what participants wrote more than conventional single-word suggestions, which were interpreted as predictions. We highlight a design challenge: how can a phrase suggestion system make valuable suggestions rather than just accurate predictions?","",7,1476776400,1476777600,540,"Author"],[172,30,"uist4549","Full","IdeaHound: improving large-scale collaborative ideation with crowd-powered real-time semantic modeling","Prior work on creativity support tools demonstrates how a computational semantic model of a solution space can enable interventions that substantially improve the number, quality and diversity of ideas. However, automated semantic modeling often falls short when people contribute short text snippets or sketches. Innovation platforms can employ humans to provide semantic judgments to construct a semantic model, but this relies on external workers completing a large number of tedious micro tasks. This requirement threatens both accuracy (external workers may lack expertise and context to make accurate semantic judgments) and scalability (external workers are costly). In this paper, we introduce IdeaHound, an ideation system that seamlessly integrates the task of defining semantic relationships among ideas into the primary task of idea generation. The system combines implicit human actions with machine learning to create a computational semantic model of the emerging solution space. The integrated nature of these judgments allows IdeaHound to leverage the expertise and efforts of participants who are already motivated to contribute to idea generation, overcoming the issues of scalability inherent to existing approaches. Our results show that participants were equally willing to use (and just as productive using) IdeaHound compared to a conventional platform that did not require organizing ideas. Our integrated crowdsourcing approach also creates a more accurate semantic model than an existing crowdsourced approach (performed by external crowds). We demonstrate how this model enables helpful creative interventions: providing diverse inspirational examples, providing similar ideas for a given idea and providing a visual overview of the solution space.  \\ ","",6,1476837000,1476838200,540,"Author"],[173,30,"uist2629","Full","Boomerang: Rebounding the Consequences of Reputation Feedback on Crowdsourcing Platforms","Paid crowdsourcing platforms suffer from low-quality work and unfair rejections, but paradoxically, most workers and requesters have high reputation scores. These inflated scores, which make high-quality work and workers difficult to find, stem from social pressure to avoid giving negative feedback. We introduce Boomerang, a reputation system for crowdsourcing platforms that elicits more accurate feedback by rebounding the consequences of feedback directly back onto the person who gave it. With Boomerang, requesters find that their highly-rated workers gain earliest access to their future tasks, and workers find tasks from their highly-rated requesters at the top of their task feed. Field experiments verify that Boomerang causes both workers and requesters to provide feedback that is more closely aligned with their private opinions. Inspired by a game-theoretic notion of incentive-compatibility, Boomerang opens opportunities for interaction design to incentivize honest reporting over strategic dishonesty. \\ ","",6,1476838200,1476839400,540,"Author"],[174,30,"uist2440","Full","Habitsourcing: Sensing the Environment through Immersive, Habit-Building Experiences","Citizen science and communitysensing applications allow everyday citizens to collect data about the physical world to benefit science and society. Yet despite successes, current approaches are still limited by the number of domain-interested volunteers who are willing and able to contribute useful data. In this paper we introduce habitsourcing, an alternative approach that harnesses the habit-building practices of millions of people to collect environmental data. To support the design and development of habitsourcing apps, we present (1) interaction techniques and design principles for sensing through actuation, a method for acquiring sensing data from cued interactions; and (2) ExperienceKit, an iOS library that makes it easy for developers to build and test habitsourcing applications. In two experiments, we show that our two proof-of-concept apps, ZenWalk and Zombies Interactive, compare favorably to their non-data collecting counterparts, and that we can effectively extract environmental data using simple detection techniques.  ","",6,1476839400,1476840600,540,"Author"],[175,30,"uist1584","Full","VizLens: A Robust and Interactive Screen Reader for Interfaces in the Real World","The world is full of physical interfaces that are inaccessible to blind people, from microwaves and information kiosks to thermostats and checkout terminals. Blind people cannot independently use such devices without at least first learning their layout, and usually only after labeling them with sighted assistance. We introduce VizLens - an accessible mobile application and supporting backend that can robustly and interactively help blind people use nearly any interface they encounter. VizLens users capture a photo of an inaccessible interface and send it to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface to make subsequent computer vision easier. The VizLens application helps users recapture the interface in the field of the camera, and uses computer vision to interactively describe the part of the interface beneath their finger (updating 8 times per second). We show that VizLens provides accurate and usable real-time feedback in a study with 10 blind participants, and our crowdsourcing labeling workflow was fast (8 minutes), accurate (99.7%), and cheap ($1.15). We then explore extensions of VizLens that allow it to (i) adapt to state changes in dynamic interfaces, (ii) combine crowd labeling with OCR technology to handle dynamic displays, and (iii) benefit from head-mounted cameras. VizLens robustly solves a long-standing challenge in accessibility by deeply integrating crowdsourcing and computer vision, and foreshadows a future of increasingly powerful interactive applications that would be currently impossible with either alone.","",6,1476840600,1476841800,540,"Author"],[176,31,"uist4574","Full","Aesthetic Electronics: Designing, Sketching, and Fabricating Circuits through Digital Exploration","As interactive electronics become increasingly intimate and personal, the design of circuitry is correspondingly developing a more playful and creative aesthetic. Circuit sketching and design is a multidimensional activity which combines the arts, crafts, and engineering broadening participation of electronic creation to include makers of diverse backgrounds. In order to support this design ecology, we present Ellustrate, a digital design tool that enables the functional and aesthetic design of electronic circuits with multiple conductive and dielectric materials. Ellustrate guides users through the fabrication and debugging process, easing the task of practical circuit creation while supporting designers' aesthetic decisions throughout the circuit authoring workflow. In a formal user study, we demonstrate how Ellustrate enables a new electronic design conversation that combines electronics, materials, and visual aesthetic concerns. \\ ","",7,1476837000,1476838200,540,"Author"],[177,31,"uist3876","Full","The Toastboard: Ubiquitous Instrumentation and Automated Checking of Breadboarded Circuits","The recent proliferation of easy to use electronic components and toolkits has introduced a large number of novices to designing and building electronic projects. Nevertheless, debugging circuits remains a difficult and time-consuming task. This paper presents a novel debugging tool for electronic design projects, the Toastboard, that aims to reduce debugging time by improving upon the standard paradigm of point-wise circuit measurements. Ubiquitous instrumentation allows for immediate visualization of an entire breadboard's state, meaning users can diagnose problems based on a wealth of data instead of having to form a single hypothesis and plan before taking a measurement. Basic connectivity information is displayed visually on the circuit itself and quantitative data is displayed on the accompanying web interface. Software-based testing functions further lower the expertise threshold for efficient debugging by diagnosing classes of circuit errors automatically. In an informal study, participants found the detailed, pervasive, and context-rich data from our tool helpful and potentially time-saving. \\ ","",7,1476838200,1476839400,540,"Author"],[178,31,"uist2213","Full","CircuitStack: Supporting Rapid Prototyping and Evolution of Electronic Circuits","For makers and developers, circuit prototyping is an integral part of building electronic projects. Currently, it is common to build circuits based on breadboard schematics that are available on various maker and DIY websites. Some breadboard schematics are used as is without modification, and some are modified and extended to fit specific needs. In such cases, diagrams and schematics merely serve as blueprints and visual instructions, but users still must physically wire the breadboard connections, which can be time-consuming and error-prone. We present CircuitStack, a system that combines the flexibility of breadboarding with the correctness of printed circuits, for enabling rapid and extensible circuit construction. This hybrid system enables circuit reconfigurability, component reusability, and high efficiency at the early stage of prototyping development.","",7,1476839400,1476840600,540,"Author"],[179,31,"uist1820","Full","Stretchis: Fabricating Highly Stretchable User Interfaces","Recent advances in materials science research allow production of highly stretchable sensors and displays. Such technologies, however, are still not accessible to non-expert makers. \\ We present a novel and inexpensive fabrication method for creating Stretchis, highly stretchable user interfaces that combine sensing capabilities and visual output. We use Polydimethylsiloxan (PDMS) as the base material for a Stretchi and show how to embed stretchable touch and proximity sensors and stretchable electroluminescent displays. Stretchis can be ultra-thin, flexible, and fully customizable, enabling non-expert makers to add interaction to elastic physical objects, shape-changing surfaces, fabrics, and the human body. We demonstrate the usefulness of our approach with three application examples that range from ubiquitous computing to wearables and on-skin interaction.","",7,1476840600,1476841800,540,"Author"],[180,33,"uist2740","Full","Optical Marionette: Graphical Manipulation of Human's Walking Direction","We present a novel manipulation method that subconsciously changes the walking direction of users via visual processing on a head mounted display (HMD). Unlike existing navigation systems that require users to recognize information and then follow directions as two separate, conscious processes, the proposed method guides users without them needing to pay attention to the information provided by the navigation system and also allows them to be graphically manipulated by controllers. In the proposed system, users perceive the real world by means of stereo images provided by a stereo camera and the HMD. Specifically, while walking, the navigation system provides users with real-time feedback by processing the images they have just perceived and giving them visual stimuli. This study examined three image-processing methods: see-through image without processing, moving stripe pattern, and changing focal region. Experimental results indicate that the changing focal region method most effectively leads walkers as it changes their walking path by approximately 200 mm/m on average. \\ ","",6,1476843600,1476844800,540,"Author"],[181,33,"uist2202","Full","NormalTouch and TextureTouch: High-fidelity 3D Haptic Shape Rendering on Handheld Virtual Reality Controllers","We present an investigation of mechanically-actuated hand�lheld controllers that render the shape of virtual objects through physical shape displacement, enabling users to feel 3D surfaces, textures, and forces that match the visual rendering. We demonstrate two such controllers, NormalTouch and TextureTouch, which are tracked in 3D and produce spatially-registered haptic feedback to a user�� finger. NormalTouch haptically renders object surfaces and provides force feedback using a tiltable and extrudable platform. TextureTouch renders the shape of virtual objects including detailed surface structure through a 4�4 matrix of actuated pins. By moving our controllers around while keeping their finger on the actuated platform, users obtain the impression of a much larger 3D shape by cognitively integrating output sensations over time. Our evaluation compares the effectiveness of our controllers with the two de-facto standards in Virtual Reality controllers: device vibration and visual feedback only. We find that haptic feedback significantly increases the accuracy of V VR interaction, most effectively by rendering high-fidelity shape output as in the case of our controllers.","",6,1476844800,1476846000,540,"Author"],[182,33,"uist1607","Full","Immersive Scuba Diving Simulator Using Virtual Reality","We present Amphibian, a simulator to experience scuba diving virtually in a terrestrial setting. While existing diving simulators mostly focus on visual and aural displays, Amphibian simulates a wider variety of sensations experienced underwater. Users rest their torso on a motion platform to feel buoyancy. Their outstretched arms and legs are placed in a suspended harness to simulate drag as they swim. An Oculus Rift head-mounted display (HMD) and a pair of headphones delineate the visual and auditory ocean scene. Additional senses simulated in Amphibian are breath motion, temperature changes, and tactile feedback through various sensors. Twelve experienced divers compared Amphibian to real-life scuba diving. We analyzed the system factors that influenced the users� sense of being there while using our simulator. We present future UI improvements for enhancing immersion in VR diving simulators.","",6,1476846000,1476847200,540,"Author"],[183,34,"uist4728","Full","Energy-Brushes: Interactive Tools for Illustrating Stylized Elemental Dynamics","Dynamic effects such as waves, splashes, fire, smoke, and explosions are an integral part of stylized animations. However, such dynamics are challenging to produce, as manually sketching key-frames requires significant effort and artistic expertise while physical simulation tools lack sufficient expressiveness and user control. We present an interactive interface for designing these elemental dynamics for animated illustrations. Users draw with coarse-scale energy brushes which serve as control gestures to drive detailed flow particles which represent local velocity fields. These fields can convey both realistic and artistic effects based on user specification. This painting metaphor for creating elemental dynamics simplifies the process, providing artistic control, and preserves the fluidity of sketching. Our system is fast, stable, and intuitive. An initial user evaluation shows that even novice users with no prior animation experience can create intriguing dynamics using our system.","",7,1476843600,1476844800,540,"Author"],[184,34,"uist4633","Full","ERICA: Interaction Mining Mobile Apps","Design plays an important role in adoption of apps. App design, however, is a complex process with multiple design activities. To enable data-driven app design applications, we present interaction mining -- capturing both static (UI layouts, visual details) and dynamic (user flows, motion details) components of an app's design. We present ERICA, a system that takes a scalable, human-computer approach to interaction mining existing Android apps without the need to modify them in any way. As users interact with apps through ERICA, it detects UI changes, seamlessly records multiple data-streams in the background, and unifies them into a user interaction trace. Using ERICA we collected interaction traces from over a thousand popular Android apps. Leveraging this trace data, we built machine learning classifiers to detect elements and layouts indicative of 23 common user flows. User flows are an important component of UX design and consists of a sequence of UI states that represent semantically meaningful tasks such as searching or composing. With these classifiers, we identified and indexed more than 3000 flow examples, and released the largest online search engine of user flows in Android apps.","",7,1476844800,1476846000,540,"Author"],[185,34,"uist4274","Full","The Elements of Fashion Style","The outfits people wear contain latent fashion concepts capturing styles, seasons, events, and environments. Fashion theorists have proposed that these concepts are shaped by design elements such as color, material, and silhouette. A dress may be “bohemian� because of its pattern, material, trim, or some combination of them: it is not always clear how low-level elements translate to high-level styles. In this paper, we use polylingual topic modeling to learn latent fashion concepts jointly in two languages capturing these elements and styles. Using this latent topic formation we can translate between these two languages through topic space, exposing the elements of fashion style. We train the polylingual topic model (PLTM) on a set of more than half a million outfits collected from Polyvore, a popular fashion-based social net- work. We present novel, data-driven fashion applications that allow users to express their needs in natural language just as they would to a real stylist and produce tailored item recommendations for these style needs.","",7,1476846000,1476847200,540,"Author"],[186,34,"uist2549","Full","Multi-Device Storyboards for Cinematic Narratives in VR","Virtual Reality (VR) narratives have the unprecedented potential to connect with an audience through presence, placing viewers within the narrative. The onset of consumer VR has resulted in an explosion of interest in immersive storytelling. Planning narratives for VR, however, is a grand challenge due to its unique affordances, its evolving cinematic vocabulary, and most importantly the lack of supporting tools to explore the creative process in VR. \\ In this paper, we distill key considerations with the planning process for VR stories, collected through a formative study conducted with film industry professionals. Based on these insights we propose a workflow, specific to the needs of professionals creating storyboards for VR film, and present a multi-device (tablet and head-mounted display) storyboard tool supporting this workflow. We discuss our design and report on feedback received from interviews following demonstration of our tool to VR film professionals. \\ ","",7,1476847200,1476848400,540,"Author"],[187,37,"uist3903","Full","SketchingWithHands: 3D Sketching Handheld Products with First-Person Hand Posture","We present SketchingWithHands, a 3D sketching system that incorporates a hand-tracking sensor. The system enables product designers to easily capture desired hand postures from a first-person point of view at any time and to use the captured hand information to explore handheld product concepts by 3D sketching while keeping the proper scale and usage of the products. Based on the analysis of design practices and drawing skills in the art and design literature, we suggest novel ideas for efficiently acquiring hand postures (palm-pinning widget, front and center mirrors, responsive spangles), for quickly creating and easily adjusting sketch planes (modified tick-triggered, orientable and shiftable sketch planes), for appropriately starting 3D sketching products with hand information (hand skeleton, grip axis), and for practically increasing user throughput (intensifier, rough and precise erasers)—all of which are coherently and consistently integrated in our system. A user test by ten industrial design students and an in-depth discussion show that our system is both useful and usable in designing handheld products.","",6,1476855000,1476856200,540,"Author"],[188,37,"uist3520","Full","Authoring Illustrations of Human Movements by Iterative Physical Demonstration","Illustrations of human movements are used to communicate ideas and convey instructions in many domains, but creating them is time-consuming and requires skill. We introduce DemoDraw, a multi-modal approach to generate these illustrations as the user physically demonstrates the movements. In a Demonstration Interface, DemoDraw segments speech and 3D joint motion into a sequence of motion segments, each characterized by a key pose and salient joint trajectories. Based on this sequence, a series of illustrations is automatically generated using a stylistically rendered 3D avatar annotated with arrows to convey movements. During demonstration, the user can navigate using speech and amend or re-perform motions if needed. Once a suitable sequence of steps has been created, a Refinement Interface enables fine control of visualization parameters. In a three-part evaluation, we validate the effectiveness of the generated illustrations and the usability of DemoDraw. Our results show 4 to 7-step illustrations can be created in 5 or 10 minutes on average.","",6,1476856200,1476857400,540,"Author"],[189,37,"uist2498","Full","AggreGaze: Collective Estimation of Audience Attention on Public Displays","Gaze is frequently explored in public display research given its importance for monitoring and analysing audience attention. \\ However, current gaze-enabled public display interfaces require either special-purpose eye tracking equipment or explicit personal calibration for each individual user. \\ We present AggreGaze, a novel method for estimating spatio-temporal audience attention on public displays. \\ Our method requires only a single off-the-shelf camera attached to the display, does not require any personal calibration, and provides visual attention estimates across the full display. \\ We achieve this by 1) compensating for errors of state-of-the-art appearance-based gaze estimation methods through on-site training data collection, and by 2) aggregating uncalibrated and thus inaccurate gaze estimates of multiple users into joint attention estimates. \\ We propose different visual stimuli for this compensation: a standard 9-point calibration, moving targets, text and visual stimuli embedded into the display content, as well as normal video content. \\ Based on a two-week deployment in a public space, we demonstrate the effectiveness of our method for estimating attention maps that closely resemble ground-truth audience gaze distributions. ","",6,1476857400,1476858600,540,"Author"],[190,37,"uist1366","Full","RadarCat: Radar Categorization for Input & Interaction","In RadarCat we present a small, versatile radar-based system for material and object classification which enables new forms of everyday proximate interaction with digital devices. We demonstrate that we can train and classify different types of materials and objects which we can then recognize in real time. Based on established research designs, we report on the results of three studies, first with 26 materials (including complex composite objects), next with 16 transparent materials (with different thickness and varying dyes) and finally 10 body parts from 6 participants. Both leave one-out and 10-fold cross-validation demonstrate that our approach of classification of radar signals using random forest classifier is robust and accurate. We further demonstrate four working examples including a physical object dictionary, painting and photo editing application, body shortcuts and automatic refill based on RadarCat. We conclude with a discussion of our results, limitations and outline future directions.","",6,1476858600,1476859800,540,"Author"],[191,38,"uist4326","Full","Advancing Hand Gesture Recognition with High Resolution Electrical Impedance Tomography","Electrical Impedance Tomography (EIT) was recently employed in the HCI domain to detect hand gestures using an instrumented smartwatch. This prior work demonstrated great promise for non-invasive, high accuracy recognition of gestures for interactive control. We introduce a new system that offers improved sampling speed and resolution. In turn, this enables superior interior reconstruction and gesture recognition. More importantly, we use our new system as a vehicle for experimentation � we compare two EIT sensing methods and three different electrode resolutions. Results from in-depth empirical evaluations and a user study shed light on the future feasibility of EIT for sensing human input. ","",7,1476855000,1476856200,540,"Author"],[192,38,"uist3820","Full","Interacting with Soli: Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum","This paper proposes a novel machine learning architecture, specifically designed for radio-frequency based gesture recognition. We focus on high-frequency (60 GHz), short-range radar based sensing, in particular Google's Soli sensor. \\ The signal has unique properties such as resolving motion at a very fine level and allowing for segmentation in range and velocity spaces rather than image space. \\ This enables recognition of new types of inputs but poses significant difficulties for the design of input recognition algorithms. \\ The proposed algorithm is capable of detecting a rich set of dynamic gestures and can resolve small motions of fingers in fine detail. \\ Our technique is based on an end-to-end trained combination of deep convolutional and recurrent neural networks. \\ The algorithm achieves high recognition rates (avg 87%) on a challenging set of 11 dynamic gestures and generalizes well across 10 users. The proposed model runs on commodity hardware at 140 Hz (CPU only).","",7,1476856200,1476857400,540,"Author"],[193,38,"uist3668","Full","WristWhirl: One-handed Continuous Smartwatch Input using Wrist Gestures","We propose and study a new input modality, WristWhirl, that uses the wrist as an always-available joystick to perform one-handed continuous input on smartwatches. We explore the influence of the wrist’s bio-mechanical properties for performing gestures to interact with a smartwatch, both while standing still and walking. Through a user study, we examine the impact of performing 8 distinct gestures (4 directional marks, and 4 free-form shapes) on the stability of the watch surface. Participants were able to perform directional marks using the wrist as a joystick at an average rate of half a second and free-form shapes at an average rate of approximately 1.5secs. The free-form shapes could be recognized by a $1 gesture recognizer with an accuracy of 93.8% and by three human inspectors with an accuracy of 85%. From these results, we designed and implemented a proof-of-concept device by augmenting the watchband using an array of proximity sensors, which can be used to draw gestures with high quality. Finally, we demonstrate a number of scenarios that benefit from one-handed continuous input on smartwatches using WristWhirl.","",7,1476857400,1476858600,540,"Author"],[194,38,"uist2105","Full","A Rapid Prototyping Approach to Synthetic Data Generation for Improved 2D Gesture Recognition","Training gesture recognizers with synthetic data generated from real gestures is a well known and powerful technique that can significantly improve recognition accuracy. In this paper we introduce a novel technique called gesture path stochastic resampling (GPSR) that is computationally efficient, has minimal coding overhead, and yet despite its simplicity is able to achieve higher accuracy than competitive, state-of-the-art approaches. GPSR generates synthetic samples by lengthening and shortening gesture subpaths within a given sample to produce realistic variations of the input via a process of nonuniform resampling. As such, GPSR is an appropriate rapid prototyping technique where ease of use, understandability, and efficiency are key. Further, through an extensive evaluation, we show that accuracy significantly improves when gesture recognizers are trained with GPSR synthetic samples. In some cases, mean recognition errors are reduced by more than 70%, and in most cases, GPSR outperforms two other evaluated state-of-the-art methods.","",7,1476858600,1476859800,540,"Author"],[195,40,"keynote2","Keynote","ambient","In this age, the guiding principle of designing an artifact is that it is a �gsingle molecule that contributes to the atmosphere of the whole environment�h.\rThat is, the creative concept of design seems to lead to defining or embodying the subject by its surroundings, rather than designing the subject itself.","",6,1476862200,1476865800,540,"Author"]]},"event_annotations":{"headers":["event_fk","annotation_fk"]},"event_attachments":{"headers":["type","event_fk","filename","directory","url"]},"event_events":{"headers":["parent_fk","child_fk","sequence"],"rows":[[14,42,1],[14,43,2],[14,44,3],[14,45,4],[14,46,5],[14,47,6],[14,48,7],[14,49,8],[14,50,9],[14,51,10],[14,52,11],[14,53,12],[14,54,13],[14,55,14],[14,56,15],[14,57,16],[14,58,17],[14,59,18],[14,60,19],[14,61,20],[14,62,21],[14,63,22],[14,64,23],[14,65,24],[14,66,25],[14,67,26],[14,68,27],[14,69,28],[14,70,29],[14,71,30],[14,72,31],[14,73,32],[14,74,33],[14,75,34],[14,76,35],[14,77,36],[14,78,37],[14,79,38],[14,80,39],[14,81,40],[14,82,41],[14,83,42],[14,84,43],[14,85,44],[27,86,1],[27,87,2],[27,88,3],[27,89,4],[27,90,5],[27,91,6],[27,92,7],[27,93,8],[27,94,9],[27,95,10],[27,96,11],[27,97,12],[27,98,13],[27,99,14],[27,100,15],[27,101,16],[27,102,17],[27,103,18],[27,104,19],[27,105,20],[27,106,21],[27,107,22],[27,108,23],[27,109,24],[27,110,25],[27,111,26],[27,112,27],[27,113,28],[27,114,29],[4,115,1],[6,116,1],[6,117,2],[6,118,3],[6,119,4],[7,120,1],[7,121,2],[7,122,3],[7,123,4],[9,124,1],[9,125,2],[9,126,3],[9,127,4],[10,128,1],[10,129,2],[10,130,3],[10,131,4],[12,132,1],[12,133,2],[12,134,3],[12,135,4],[12,136,5],[13,137,1],[13,138,2],[13,139,3],[13,140,4],[16,141,1],[16,142,2],[16,143,3],[16,144,4],[17,145,1],[17,146,2],[17,147,3],[17,148,4],[19,149,1],[19,150,2],[19,151,3],[19,152,4],[20,153,1],[20,154,2],[20,155,3],[20,156,4],[20,157,5],[22,158,1],[22,159,2],[22,160,3],[23,161,1],[23,162,2],[23,163,3],[25,164,1],[25,165,2],[25,166,3],[25,167,4],[26,168,1],[26,169,2],[26,170,3],[26,171,4],[30,172,1],[30,173,2],[30,174,3],[30,175,4],[31,176,1],[31,177,2],[31,178,3],[31,179,4],[33,180,1],[33,181,2],[33,182,3],[34,183,1],[34,184,2],[34,185,3],[34,186,4],[37,187,1],[37,188,2],[37,189,3],[37,190,4],[38,191,1],[38,192,2],[38,193,3],[38,194,4],[40,195,1]]},"event_people":{"headers":["event_fk","person_fk","sequence"],"rows":[[3,1,1],[4,2,1],[6,3,1],[7,4,1],[9,5,1],[10,6,1],[12,7,1],[13,8,1],[16,9,1],[17,10,1],[19,11,1],[20,12,1],[22,13,1],[23,14,1],[25,15,1],[26,16,1],[30,17,1],[31,18,1],[33,19,1],[34,20,1],[37,21,1],[38,22,1],[40,23,1],[41,24,1],[42,25,1],[43,26,1],[43,27,2],[43,28,3],[43,29,4],[43,30,5],[43,31,6],[43,32,7],[44,33,1],[44,34,2],[44,35,3],[45,36,1],[45,37,2],[45,38,3],[45,39,4],[45,40,5],[45,41,6],[46,42,1],[46,43,2],[47,44,1],[47,45,2],[48,46,1],[48,45,2],[49,47,1],[49,48,2],[49,49,3],[50,50,1],[50,51,2],[50,52,3],[50,53,4],[51,54,1],[51,55,2],[51,56,3],[52,57,1],[52,58,2],[52,59,3],[52,60,4],[52,61,5],[53,62,1],[53,63,2],[53,64,3],[53,65,4],[53,66,5],[53,67,6],[53,68,7],[53,69,8],[54,70,1],[54,71,2],[54,72,3],[54,73,4],[54,74,5],[54,75,6],[54,76,7],[54,77,8],[55,78,1],[55,79,2],[55,80,3],[55,40,4],[55,36,5],[55,41,6],[56,81,1],[56,82,2],[56,83,3],[56,84,4],[57,85,1],[57,86,2],[57,87,3],[57,88,4],[57,89,5],[58,90,1],[58,44,2],[58,45,3],[59,91,1],[59,92,2],[59,93,3],[59,94,4],[59,95,5],[59,96,6],[72,91,1],[72,96,2],[60,97,1],[60,98,2],[60,99,3],[60,100,4],[60,101,5],[60,102,6],[61,103,1],[61,104,2],[61,105,3],[61,106,4],[61,107,5],[61,102,6],[61,108,7],[62,109,1],[62,110,2],[63,111,1],[63,112,2],[63,113,3],[64,114,1],[64,115,2],[64,116,3],[65,117,1],[65,56,2],[66,118,1],[66,116,2],[67,119,1],[67,120,2],[67,121,3],[68,122,1],[68,43,2],[69,123,1],[69,124,2],[70,125,1],[70,126,2],[70,127,3],[70,128,4],[71,129,1],[71,124,2],[73,130,1],[73,45,2],[74,131,1],[74,132,2],[74,133,3],[74,134,4],[74,135,5],[74,136,6],[74,77,7],[75,137,1],[75,138,2],[75,139,3],[76,140,1],[76,141,2],[76,142,3],[76,143,4],[76,95,5],[76,144,6],[76,145,7],[77,146,1],[77,147,2],[78,148,1],[78,149,2],[79,109,1],[79,150,2],[79,110,3],[80,151,1],[80,152,2],[80,153,3],[80,154,4],[80,155,5],[81,156,1],[81,157,2],[81,158,3],[81,113,4],[81,159,5],[81,160,6],[82,161,1],[82,162,2],[83,163,1],[83,164,2],[83,165,3],[83,166,4],[83,167,5],[84,168,1],[84,169,2],[84,170,3],[84,171,4],[85,172,1],[85,173,2],[85,174,3],[86,175,1],[87,176,1],[87,177,2],[87,178,3],[87,179,4],[88,180,1],[88,158,2],[88,181,3],[88,182,4],[89,183,1],[89,184,2],[89,185,3],[89,186,4],[89,187,5],[90,188,1],[90,189,2],[90,190,3],[90,191,4],[91,192,1],[91,193,2],[91,194,3],[92,195,1],[92,196,2],[92,197,3],[92,198,4],[92,199,5],[93,200,1],[93,201,2],[93,202,3],[94,203,1],[94,54,2],[94,56,3],[95,204,1],[95,205,2],[95,206,3],[95,207,4],[96,208,1],[96,209,2],[96,210,3],[97,211,1],[97,212,2],[97,213,3],[97,214,4],[98,215,1],[98,216,2],[99,217,1],[99,114,2],[99,218,3],[99,116,4],[100,219,1],[100,220,2],[101,221,1],[101,222,2],[101,223,3],[101,116,4],[102,224,1],[102,225,2],[103,226,1],[103,227,2],[104,228,1],[105,229,1],[105,230,2],[105,231,3],[105,232,4],[105,233,5],[105,234,6],[106,235,1],[106,236,2],[106,237,3],[106,238,4],[107,239,1],[107,240,2],[107,241,3],[107,242,4],[107,243,5],[108,244,1],[108,245,2],[108,49,3],[109,246,1],[109,247,2],[109,248,3],[110,249,1],[110,250,2],[110,167,3],[111,251,1],[111,252,2],[112,253,1],[112,254,2],[112,255,3],[112,256,4],[112,257,5],[113,258,1],[113,259,2],[113,260,3],[113,261,4],[113,262,5],[113,263,6],[114,264,1],[114,265,2],[114,266,3],[114,267,4],[114,268,5],[115,269,1],[116,270,1],[116,271,2],[116,272,3],[116,273,4],[116,274,5],[117,275,1],[117,276,2],[117,277,3],[117,278,4],[117,279,5],[117,280,6],[117,281,7],[117,282,8],[117,283,9],[117,284,10],[117,285,11],[117,286,12],[117,287,13],[117,288,14],[117,289,15],[117,290,16],[118,291,1],[118,292,2],[118,293,3],[118,276,4],[118,294,5],[118,295,6],[119,296,1],[119,297,2],[119,298,3],[119,299,4],[119,300,5],[119,301,6],[119,302,7],[119,303,8],[120,304,1],[120,305,2],[120,306,3],[120,307,4],[120,308,5],[121,309,1],[121,310,2],[121,311,3],[122,312,1],[122,313,2],[122,170,3],[122,171,4],[123,314,1],[123,315,2],[123,316,3],[123,317,4],[124,318,1],[124,319,2],[124,124,3],[124,166,4],[124,320,5],[125,321,1],[125,322,2],[125,323,3],[125,324,4],[125,325,5],[125,124,6],[126,319,1],[126,326,2],[126,123,3],[126,327,4],[126,328,5],[126,166,6],[126,263,7],[126,124,8],[127,329,1],[127,330,2],[127,331,3],[127,332,4],[127,333,5],[127,334,6],[127,320,7],[128,335,1],[128,336,2],[128,53,3],[128,337,4],[129,338,1],[129,339,2],[129,340,3],[130,341,1],[130,342,2],[131,343,1],[131,344,2],[131,345,3],[131,346,4],[132,347,1],[132,348,2],[133,349,1],[133,213,2],[133,350,3],[133,351,4],[134,352,1],[134,353,2],[134,354,3],[134,274,4],[135,338,1],[135,355,2],[135,356,3],[135,357,4],[135,358,5],[135,340,6],[136,304,1],[136,359,2],[136,360,3],[136,167,4],[136,308,5],[137,361,1],[137,362,2],[138,363,1],[138,364,2],[138,365,3],[138,366,4],[138,367,5],[139,368,1],[139,369,2],[140,370,1],[140,371,2],[140,372,3],[140,373,4],[140,374,5],[140,375,6],[140,376,7],[140,377,8],[140,378,9],[140,379,10],[140,380,11],[140,381,12],[140,382,13],[140,383,14],[140,384,15],[140,385,16],[141,386,1],[141,387,2],[141,388,3],[141,389,4],[141,390,5],[142,391,1],[142,392,2],[142,393,3],[142,394,4],[143,395,1],[143,396,2],[144,397,1],[144,398,2],[145,316,1],[145,399,2],[145,317,3],[191,315,1],[191,399,2],[191,317,3],[146,400,1],[146,401,2],[146,402,3],[146,403,4],[146,404,5],[146,405,6],[146,406,7],[146,407,8],[146,408,9],[147,409,1],[147,410,2],[147,411,3],[147,412,4],[147,413,5],[147,414,6],[147,415,7],[148,291,1],[148,416,2],[149,417,1],[149,418,2],[149,419,3],[149,420,4],[149,421,5],[150,422,1],[150,423,2],[151,424,1],[151,425,2],[151,426,3],[152,427,1],[152,428,2],[152,429,3],[152,430,4],[153,431,1],[153,432,2],[153,433,3],[154,434,1],[154,435,2],[154,436,3],[155,48,1],[155,437,2],[155,438,3],[155,439,4],[155,440,5],[155,49,6],[156,441,1],[156,442,2],[156,443,3],[156,444,4],[157,68,1],[157,445,2],[157,446,3],[158,447,1],[158,448,2],[158,449,3],[158,450,4],[158,451,5],[159,452,1],[159,233,2],[159,453,3],[159,454,4],[159,234,5],[160,455,1],[160,456,2],[160,457,3],[161,458,1],[161,459,2],[161,460,3],[161,461,4],[162,462,1],[162,460,2],[162,463,3],[163,464,1],[163,465,2],[163,466,3],[163,461,4],[164,467,1],[164,468,2],[164,469,3],[164,470,4],[164,471,5],[164,472,6],[164,352,7],[164,473,8],[164,274,9],[165,474,1],[165,475,2],[165,476,3],[165,477,4],[165,478,5],[166,479,1],[166,354,2],[166,480,3],[166,295,4],[167,481,1],[167,482,2],[167,483,3],[167,484,4],[168,485,1],[168,486,2],[168,487,3],[168,488,4],[168,489,5],[168,490,6],[169,491,1],[169,492,2],[169,53,3],[170,493,1],[170,494,2],[170,495,3],[170,496,4],[170,209,5],[170,497,6],[171,498,1],[171,499,2],[171,426,3],[172,500,1],[172,501,2],[172,502,3],[172,499,4],[173,503,1],[173,504,2],[173,505,3],[173,506,4],[173,507,5],[173,508,6],[173,509,7],[173,510,8],[173,511,9],[173,512,10],[173,513,11],[173,514,12],[173,515,13],[173,516,14],[173,517,15],[173,369,16],[174,518,1],[174,519,2],[174,520,3],[174,521,4],[174,362,5],[175,522,1],[175,291,2],[175,523,3],[175,524,4],[175,525,5],[175,526,6],[175,527,7],[176,528,1],[176,529,2],[176,530,3],[176,531,4],[176,366,5],[176,460,6],[176,532,7],[176,533,8],[177,534,1],[177,535,2],[177,536,3],[177,537,4],[177,538,5],[177,466,6],[178,539,1],[178,540,2],[178,541,3],[178,542,4],[178,543,5],[178,496,6],[178,77,7],[178,544,8],[179,545,1],[179,546,2],[179,53,3],[180,67,1],[180,62,2],[180,547,3],[180,548,4],[180,549,5],[180,550,6],[180,69,7],[181,551,1],[181,552,2],[181,553,3],[181,554,4],[182,259,1],[182,258,2],[182,555,3],[182,556,4],[182,557,5],[182,558,6],[182,263,7],[183,559,1],[183,560,2],[183,276,3],[183,561,4],[183,562,5],[183,563,6],[184,564,1],[184,565,2],[184,566,3],[185,567,1],[185,568,2],[185,569,3],[185,570,4],[185,566,5],[186,571,1],[186,388,2],[186,572,3],[186,573,4],[186,340,5],[187,574,1],[187,575,2],[188,576,1],[188,387,2],[188,366,3],[188,460,4],[188,466,5],[189,577,1],[189,578,2],[189,579,3],[190,211,1],[190,580,2],[190,581,3],[190,582,4],[190,214,5],[192,583,1],[192,584,2],[192,585,3],[192,586,4],[192,31,5],[193,587,1],[193,588,2],[193,589,3],[194,590,1],[194,591,2],[194,592,3],[194,593,4],[195,594,1]]},"location":{"headers":["_id","name","map_x_pct","map_y_pct","sequence","map_file","map_url","map_name"],"rows":[[1,"Red Room",0.3442622950819672,0.6055555555555555,1,"","",""],[2,"Blue Room",0.5045537340619308,0.6611111111111111,2,"","",""],[3,"Green Room",0.6539162112932605,0.6083333333333333,3,"","",""],[4,"Oval Room",0.5027322404371585,0.6475409836065574,4,"","",""],[5,"",-1,-1,5,"","",""],[6,"Auditorium",0.8175046554934823,0.7626262626262627,6,"2f-map-on-program.png","https://firebasestorage.googleapis.com/v0/b/confapp-data-sync.appspot.com/o/uist_2016%2Fmaps%2F2f-map-on-program.png?alt=media&token=da06b0eb-8f47-4b7f-a47a-e80814bce3f7","2nd floor"],[7,"Ballroom",0.3854748603351955,0.7575757575757576,7,"2f-map-on-program.png","https://firebasestorage.googleapis.com/v0/b/confapp-data-sync.appspot.com/o/uist_2016%2Fmaps%2F2f-map-on-program.png?alt=media&token=da06b0eb-8f47-4b7f-a47a-e80814bce3f7","2nd floor"],[8,"Hitotsubashi hall 2F",0.6163873370577281,0.5580808080808081,8,"2f-map-on-program.png","https://firebasestorage.googleapis.com/v0/b/confapp-data-sync.appspot.com/o/uist_2016%2Fmaps%2F2f-map-on-program.png?alt=media&token=da06b0eb-8f47-4b7f-a47a-e80814bce3f7","2nd floor"],[9,"Hotel Grand Palace",-1,-1,9,"","",""],[10,"Josui Kaikan",-1,-1,10,"","",""],[11,"undefined",-1,-1,11,"","",""]]},"person":{"headers":["_id","name","last_name","affiliation"],"rows":[[1,"Takeo Igarashi","Igarashi","The University of Tokyo"],[2,"Takeo Igarashi","Igarashi","The University of Tokyo"],[3,"Stefanie Mueller","Mueller","M.I.T"],[4,"Xiaojun Bi","Bi","Google Research"],[5,"Daniel Avrahami","Avrahami","FXPAL"],[6,"Jacob Wobbrock","Wobbrock","University of Washington"],[7,"Hrvoje Benko","Benko","Microsoft Research"],[8,"Ranjitha Kumar","Kumar","University of Illinois�\\Urbana Champaign"],[9,"Seok-Hyung Bae","Bae","KAIST"],[10,"Aaron Quigley","Quigley","University of St. Andrews"],[11,"Fanny Chevalier","Chevalier","INRIA"],[12,"Nicholas Chen","Chen","Microsoft Research"],[13,"Parmit Chilana","Chilana","Simon Frasier University"],[14,"Rubiat Kazi","Kazi","Autodesk Research"],[15,"Scott Hudson","Hudson","Carnegie Mellon University"],[16,"Koji Yatani","Yatani","University of Tokyo"],[17,"Walter Lasecki","Lasecki","University of Michigan"],[18,"Otmar Hilliges","Hilliges","ETH Zurich"],[19,"Yuta Sugiura","Sugiura","Keio University"],[20,"Joel Brandt","Brandt","Adobe Research"],[21,"Justin Matejka","Matejka","Autodesk Research"],[22,"Meredith Morris","Morris","Microsoft Research"],[23,"Takeo Igarashi","Igarashi","The University of Tokyo"],[24,"Takeo Igarashi","Igarashi","The University of Tokyo"],[25,"Akira Nakayasu","Nakayasu","Kanazawa College of Art"],[26,"Oliver Glauser","Glauser","ETH Zurich"],[27,"Benedek Vartok","Vartok","ETH Zurich"],[28,"Wan-Chun Ma","Ma","Activision Inc."],[29,"Daniele Panozzo","Panozzo","New York University"],[30,"Alec Jacobson","Jacobson","Columbia University"],[31,"Otmar Hilliges","Hilliges","ETH Zurich"],[32,"Olga Sorkine-Hornung","Sorkine-Hornung","ETH Zurich"],[33,"Yuki Kubo","Kubo","University of Tsukuba"],[34,"Buntarou Shizuki","Shizuki","University of Tsukuba"],[35,"Shin Takahashi","Takahashi","University of Tsukuba"],[36,"Hyung-Sik Kim","Kim","Konkuk University"],[37,"Seong-Young Gim","Gim","Konkuk University"],[38,"Woo-Ram Kim","Kim","Konkuk University"],[39,"Mi-Hyun Choi","Choi","Konkuk University"],[40,"Seungmoon Choi","Choi","Pohang University of Science and Technology (POSTECH)"],[41,"Soon-Cheol Chung","Chung","Konkuk University"],[42,"Toshiya Yui","Yui","Waseda University"],[43,"Tomoko Hashida","Hashida","Waseda University"],[44,"Kunihiro Kato","Kato","Meiji University"],[45,"Homei Miyashita","Miyashita",""],[46,"Haruki Takahashi","Takahashi","Meiji University"],[47,"Daniel Spelmezan","Spelmezan","University of Sussex"],[48,"Deepak Ranjan Sahoo","Sahoo","University of Sussex"],[49,"Sriram Subramanian","Subramanian","University of Sussex"],[50,"Carla Griggio","Griggio","INRIA"],[51,"Nam Giang","Giang","INRIA"],[52,"Germ�かn Leiva","Leiva","INRIA"],[53,"Wendy Mackay","Mackay","Inria, Universit��? Paris-Saclay"],[54,"Xinlei Zhang","Zhang","The University of Tokyo"],[55,"Takashi Miyaki","Miyaki","The University of Tokyo"],[56,"Jun Rekimoto","Rekimoto","University of Tokyo / Sony CSL"],[57,"Mai Otsuki","Otsuki","University of Tsukuba"],[58,"Taiki Kawano","Kawano","Universty of Tsukuba"],[59,"Keita Maruyama","Maruyama","Universty of Tsukuba"],[60,"Hideaki Kuzuoka","Kuzuoka","University of Tsukuba"],[61,"Yusuke SUZUKI","SUZUKI","Oki Electric Industry Co., Ltd."],[62,"Ippei Suzuki","Suzuki","University of Tsukuba"],[63,"Shuntarou Yoshimitsu","Yoshimitsu","Waseda University"],[64,"Keisuke Kawahara","Kawahara","University of Tsukuba"],[65,"Nobutaka Ito","Ito","The University of Tokyo"],[66,"Atushi Shinoda","Shinoda","University of Tsukuba"],[67,"Akira Ishii","Ishii","University of Tsukuba"],[68,"Takatoshi Yoshida","Yoshida","The University of Tokyo"],[69,"Yoichi Ochiai","Ochiai","University of Tsukuba"],[70,"Yang-Sheng Chen","Chen","Graduate Institute of Networking and Multimedia"],[71,"Ping-Hsuan Han","Han","National Taiwan University"],[72,"Jui-Chun Hsiao","Hsiao","National Taiwan University"],[73,"Kong-Chang Lee","Lee","Tamkang"],[74,"Chiao-En Hsieh","Hsieh","National Taiwan University"],[75,"Kuan-Yin Lu","Lu","National Taiwan University"],[76,"Chien-Hsing Chou","Chou","Tamkang University"],[77,"Yi-Ping Hung","Hung","National Taiwan University"],[78,"Hojin Lee","Lee","Pohang University of Science and Technology (POSTECH)"],[79,"Hojun Cha","Cha","Pohang University of Science and Technology (POSTECH)"],[80,"Junsuk Park","Park","Pohang University of Science and Technology (POSTECH)"],[81,"Han-Jong Kim","Kim","KAIST"],[82,"Yunwoo Jeong","Jeong","KAIST"],[83,"Ju-Whan Kim","Kim","KAIST"],[84,"Tek-Jin Nam","Nam","KAIST"],[85,"Jun Shingu","Shingu","FujiXerox Co., Ltd."],[86,"Patrick Chiu","Chiu","FX Palo Alto Laboratory"],[87,"Sven Kratz","Kratz","FX Palo Alto Laboratory"],[88,"Jim Vaughan","Vaughan","FX Palo Alto Laboratory"],[89,"Don Kimber","Kimber","FX Palo Alto Laboratory"],[90,"Saraha Ueno","Ueno","Meiji University"],[91,"Nimesha Ranasinghe","Ranasinghe","National University of Singapore"],[92,"Pravar Jain","Jain","National University of Singapore"],[93,"David Tolley","Tolley","National University of Singapore"],[94,"Shienny Karwita","Karwita","National University of Singapore"],[95,"Yilei Shi","Shi","Singapore University of Technology and Design"],[96,"Ellen Yi-Luen Do","Do","NUS"],[97,"Natsuki Miyata","Miyata","National Institute of Advanced Industrial Science and Technology (AIST)"],[98,"Takehiro Honoki","Honoki","Yokohama National University"],[99,"Yusuke Maeda","Maeda","Yokohama National University"],[100,"Yui Endo","Endo","National Institute of Advanced Industrial Science and Technology"],[101,"Mitsunori Tada","Tada","National Institute of Advanced Industrial Science and Technology (AIST)"],[102,"Yuta Sugiura","Sugiura","Keio University"],[103,"Katsuhiro Suzuki","Suzuki","Keio University"],[104,"Fumihiko Nakamura","Nakamura","Keio University"],[105,"Jiu Otsuka","Otsuka","Keio University"],[106,"Katsutoshi Masai","Masai","Keio University"],[107,"Yuta Itoh","Itoh","Keio University"],[108,"Maki Sugimoto","Sugimoto","Keio University"],[109,"Kaori Ikematsu","Ikematsu","Ochanomizu University"],[110,"Itiro Siio","Siio","Ochanomizu University"],[111,"Mina Shibasaki","Shibasaki","Keio University"],[112,"Youichi Kamiyama","Kamiyama","Keio University"],[113,"Kouta Minamizawa","Minamizawa","Keio University"],[114,"Shio Miyafuji","Miyafuji","Tokyo Institute of Technology"],[115,"Masato Sugasaki","Sugasaki","Tokyo Institute of Technology"],[116,"Hideki Koike","Koike","Tokyo Institute of Technology"],[117,"Akira Matsuda","Matsuda","Interfaculty Initiative in Information Studies"],[118,"Toshiki Sato","Sato","Tokyo Institute of Technology"],[119,"Xinxiao Li","Li","Toshiba R&D Center"],[120,"Akira Kuroda","Kuroda","Toshiba R&D Center"],[121,"Hidenori Matsuzaki","Matsuzaki","Toshiba R&D Center"],[122,"Tomohiro Yokota","Yokota","Waseda University"],[123,"Inrak Choi","Choi","Stanford University"],[124,"Sean Follmer","Follmer","Stanford University"],[125,"Takefumi Hiraki","Hiraki","The University of Tokyo"],[126,"Koya Narumi","Narumi","The University of Tokyo"],[127,"Koji Yatani","Yatani","The University of Tokyo"],[128,"Yoshihiro Kawahara","Kawahara","The University of Tokyo"],[129,"Evan Strasnick","Strasnick","Stanford University"],[130,"Yoh Akiyama","Akiyama","Nakano"],[131,"Shi-Yao Wei","Wei","National Taiwan University"],[132,"Chen-Yu Wang","Wang","National Taiwan University"],[133,"Ting-Wei Chiu","Chiu","National Taiwan University"],[134,"Yi-Ping Lo","Lo","Institute for Information Industry"],[135,"Zhi-Wei Yang","Yang","National Taiwan University"],[136,"Hsing-Man Wang","Wang","Institute for Information Industry"],[137,"Junki Kikuchi","Kikuchi","School of Systems Information Science"],[138,"Hidekatsu Yanagi","Yanagi","School of Systems Information Science"],[139,"Yoshiaki Mima","Mima","School of Systems Information Science"],[140,"Anusha Withana","Withana","Singapore University of Technology and Design"],[141,"Shanaka Ransiri","Ransiri","Singapore University of Technology and Design"],[142,"Tharindu Kaluarachchi","Kaluarachchi","Singapore University of Technology and Design"],[143,"Chanaka Singhabahu","Singhabahu","Singapore University of Technology and Design"],[144,"Don Samitha Elvitigala","Elvitigala","SUTD"],[145,"Suranga Nanayakkara","Nanayakkara","Singapore University of Technology and Design"],[146,"Wataru Yamada","Yamada","NTT DOCOMO"],[147,"Hiroyuki Manabe","Manabe","NTT DOCOMO"],[148,"Ayuri Tomohiro","Tomohiro",""],[149,"Yasuyuki Sumi","Sumi","Future University Hakodate"],[150,"Mana Sasagawa","Sasagawa","Bunkyo-ku"],[151,"Yuki Asai","Asai","Osaka University"],[152,"Yuta Ueda","Ueda","Osaka University"],[153,"Ryuichi Enomoto","Enomoto","Osaka University"],[154,"Daisuke Iwai","Iwai","Osaka University"],[155,"Kosuke Sato","Sato","Osaka University"],[156,"Yukari Konishi","Konishi","Keio University"],[157,"Nobuhisa Hanamitsu","Hanamitsu","Keio University"],[158,"Benjamin Outram","Outram","Keio University"],[159,"Ayahiko Sato","Sato","Rhizomatiks Co. Ltd."],[160,"Tetsuya Mizuguchi","Mizuguchi","Keio University"],[161,"Ryosuke Kawakatsu","Kawakatsu","Graduate School of Kyoto Sangyo University"],[162,"Shigeyuki Hirai","Hirai","Kyoto Sangyo University"],[163,"Xin LIU","LIU","Massachusetts Institute of Technology"],[164,"Katia Vega","Vega","Massachusetts Institute of Technology "],[165,"Jing Qian","Qian","Brown University"],[166,"Joseph Paradiso","Paradiso","Massachusetts Institute of Technology"],[167,"Pattie Maes","Maes","Massachusetts Institute of Technology"],[168,"Antonio Gomes","Gomes","Queen's University"],[169,"Lahiru Priyadarshana","Priyadarshana","Queen's University"],[170,"Juan Pablo Carrascal","Carrascal","Queen's University"],[171,"Roel Vertegaal","Vertegaal","Human Media Lab, Queen's University."],[172,"Emi Tamaki","Tamaki","Waseda University"],[173,"Terence Chan","Chan","H2L, Inc"],[174,"Ken Iwasaki","Iwasaki","H2L, Inc"],[175,"Kyungmin Youn","Youn","Google, Inc."],[176,"Yukiko Yokomizo","Yokomizo","Tokyo Metropolitan Univercity"],[177,"Tomoya Kotegawa","Kotegawa","Graduate School of System Design"],[178,"Paul Haimes","Haimes","Tokyo Metropolitan University"],[179,"Tetsuaki Baba","Baba","Industrial Art"],[180,"Yun Suen Pai","Pai","Keio University"],[181,"Noriyasu Vontin","Vontin","Fujitsu Design Ltd."],[182,"Kai Kunze","Kunze","Keio University"],[183,"Tomomi Takashina","Takashina","Nikon Corporation"],[184,"Tsutomu Tamura","Tamura","Nikon Systems Inc."],[185,"Makoto Nakazumi","Nakazumi","Nikon Corporation"],[186,"Tatsushi Nomura","Nomura","Nikon Corporation"],[187,"Yuji Kokumai","Kokumai","Nikon Corporation"],[188,"Mitsuhito Ando","Ando","Yamaguchi Center for Arts and Media"],[189,"Chisaki Murakami","Murakami","Yamaguchi Center for Arts and Media"],[190,"Takayuki Ito","Ito","Yamaguchi Center for Arts and Media"],[191,"Kazuhiro Jo","Jo","Yamaguchi Center for Arts and Media"],[192,"Keiichi Ueno","Ueno","University of Yamanashi"],[193,"Kentaro Go","Go","University of Yamanashi"],[194,"Yuichiro Kinoshita","Kinoshita","University of Yamanashi"],[195,"Ahmed Farooq","Farooq","Tampere University"],[196,"Philipp Weitz","Weitz","University Of Tampere"],[197,"Grigori Evreinov","Evreinov","University of Tampere, Finland"],[198,"Roope Raisamo","Raisamo","University of Tampere, Finland"],[199,"Daisuke Takahata","Takahata","Fukoku CO. LTD"],[200,"Soomin Kim","Kim","Seoul National University"],[201,"JongHwan Oh","Oh","Seoul National University"],[202,"Joonhwan Lee","Lee","Seoul National University"],[203,"Shogo Yamashita","Yamashita","The University of Tokyo"],[204,"Takehiro Nagatomo","Nagatomo","Oita University"],[205,"Takahiro Tachibana","Tachibana","Oita University"],[206,"Keizo Sato","Sato","Oita University"],[207,"Makoto Nakashima","Nakashima","Oita University"],[208,"Roshan Peiris","Peiris","Keio University"],[209,"Liwei Chan","Chan","Keio University"],[210,"Kouta Minamizawa","Minamizawa","Keio University"],[211,"Hui-Shyong Yeo","Yeo","University of St Andrews"],[212,"Juyoung Lee","Lee","KAIST"],[213,"Andrea Bianchi","Bianchi","Korea Advanced Institute of Science and Technology"],[214,"Aaron Quigley","Quigley","University of St Andrews"],[215,"Handityo Aulia Putra","Putra","Kochi University of Technology"],[216,"Xiangshi Ren","Ren","Kochi University of Technology "],[217,"Zhengqing Li","Li","Tokyo Institute of Technology"],[218,"Toshiki Sato","Sato","Tokyo Institute of Technology"],[219,"Nobuhiro Funato","Funato","Tokai University"],[220,"Kentaro Takemura","Takemura","Tokai University"],[221,"Ryohei Funakoshi","Funakoshi","Tokyo Institution of technology"],[222,"Vishnu Boddeti","Boddeti","Carnegie Mellon University"],[223,"Kris Kitani","Kitani","Carnegie Mellon University"],[224,"Arinobu Niijima","Niijima","The University of Tokyo"],[225,"Takefumi Ogawa","Ogawa","The University of Tokyo"],[226,"Salem Karani","Karani","MIT Media Lab"],[227,"Chaitanya Varanasi","Varanasi","Citigroup Inc"],[228,"Pramod Verma","Verma","Flying Interface"],[229,"Han Joo Chae","Chae","Seoul National University"],[230,"Jeong-in Hwang","Hwang","Seoul National University"],[231,"Yuri Choi","Choi","Samsung Electronics"],[232,"Yieun Kim","Kim","Seoul National University"],[233,"Kyle Koh","Koh","Seoul National University"],[234,"Jinwook Seo","Seo","Seoul National University"],[235,"Jooyeon Lee","Lee","Yonsei University"],[236,"Manri Cheon","Cheon","Yonsei University"],[237,"Seong-Eun Moon","Moon","Yonsei University"],[238,"Jong-Seok Lee","Lee","Yonsei University"],[239,"Yu-Kai Chiu","Chiu","National Taiwan University"],[240,"Hao-Yu Chang","Chang","National Taiwan University"],[241,"Wan-ling Yang","Yang","Graduate Institute of Networking and Multimedia"],[242,"Yu-Hsuan Huang","Huang","National Taiwan University"],[243,"Ouhyoung Ming","Ming","National Taiwan University"],[244,"William Frier","Frier","University of Sussex"],[245,"Kyoungwon Seo","Seo","Hanyang University"],[246,"Eleuda Nunez","Nunez","University of Tsukuba"],[247,"Francesco Visentin","Visentin","University of Tsukuba"],[248,"Kenji Suzuki","Suzuki","University of Tsukuba"],[249,"Oscar Rosello","Rosello","Massachusetts Institute of Technology"],[250,"Marc Exposito","Exposito","MIT"],[251,"Jihye Lee","Lee","Sungkyunkwan University"],[252,"Sangwon Lee","Lee","Sungkyunkwan University"],[253,"Hidde van der Meulen","van der Meulen","Utrecht University"],[254,"Petra Varsanyi","Varsanyi","University of New Hampshire"],[255,"Lauren Westendorf","Westendorf","Wellesley College"],[256,"Andrew Kun","Kun","University of New Hampshire"],[257,"Orit Shaer","Shaer","Wellesley College"],[258,"Misha Sra","Sra","Massachusetts Institute of Technology"],[259,"Dhruv Jain","Jain","Massachusetts Institute of Technology"],[260,"Arthur Pitzer Caetano","Caetano","Universidade Federal Fluminense"],[261,"Andres Calvo","Calvo","Massachusetts Institute of Technology"],[262,"Erwin Hilton","Hilton","MIT"],[263,"Chris Schmandt","Schmandt","Massachusetts Institute of Technology"],[264,"Hyunchul Lim","Lim","Seoul National University"],[265,"Jungmin Chung","Chung","Seoul National University"],[266,"Changhoon Oh","Oh","Seoul National University"],[267,"SoHyun Park","Park","Seoul National University"],[268,"Bongwon Suh","Suh","Seoul National University"],[269,"Takeo Kanade","Kanade","Carnegie Mellon University"],[270,"Thijs Jan Roumen","Roumen","Hasso Plattner Institute"],[271,"Bastian Kruck","Kruck","Hasso Plattner Institute"],[272,"Tobias Dürschmid","Dürschmid","Hasso Plattner Institute"],[273,"Tobias Nack","Nack","Hasso Plattner Institute"],[274,"Patrick Baudisch","Baudisch","Hasso Plattner Institute"],[275,"Benjamin Lafreniere","Lafreniere","Autodesk Research"],[276,"Tovi Grossman","Grossman","Autodesk Research"],[277,"Fraser Anderson","Anderson","Autodesk Research"],[278,"Justin Matejka","Matejka","Autodesk Research"],[279,"Heather Kerrick","Kerrick","Autodesk"],[280,"Danil Nagy","Nagy","Autodesk"],[281,"Lauren Vasey","Vasey","University of Stuttgart"],[282,"Evan Atherton","Atherton","Autodesk"],[283,"Nicholas Beirne","Beirne","Autodesk Research"],[284,"Marcelo H Coelho","Coelho","Marcelo Coelho Studio"],[285,"Nicholas Cote","Cote","Autodesk"],[286,"Steven Li","Li","Autodesk Research"],[287,"Andy Nogueira","Nogueira","Autodesk Research"],[288,"Long Nguyen","Nguyen","University of Stuttgart"],[289,"Tobias Schwinn","Schwinn","University of Stuttgart"],[290,"James Stoddart","Stoddart","Autodesk"],[291,"Xiang 'Anthony' Chen","Chen","Carnegie Mellon University"],[292,"Jeeeun Kim","Kim","University of Colorado, Boulder"],[293,"Jennifer Mankoff","Mankoff","Carnegie Mellon University"],[294,"Stelian Coros","Coros","Carnegie Mellon University"],[295,"Scott E Hudson","Hudson","Carnegie Mellon"],[296,"Tobias Grosse-Puppendahl","Grosse-Puppendahl","Microsoft Research"],[297,"Steve Hodges","Hodges","Microsoft Research"],[298,"Nicholas Chen","Chen",""],[299,"John Helmes","Helmes","Microsoft Research "],[300,"Stuart Taylor","Taylor","Microsoft Research, Cambridge"],[301,"James Scott","Scott","Microsoft Research"],[302,"Josh Fromm","Fromm","University of Washington"],[303,"David Sweeney","Sweeney","Microsoft Research"],[304,"Jan Gugenheimer","Gugenheimer","Ulm University"],[305,"David Dobbelstein","Dobbelstein","Ulm University"],[306,"Christian Winkler","Winkler","Ulm University"],[307,"Gabriel Haas","Haas","Ulm University"],[308,"Enrico Rukzio","Rukzio","Institute of Media Informatics"],[309,"Joseph Chee Chang","Chang","Carnegie Mellon University"],[310,"Nathan Hahn","Hahn","Human-Computer Interaction Institute"],[311,"Aniket Kittur","Kittur","Carnegie Mellon University"],[312,"Daniel Gotsch","Gotsch","Queen's University"],[313,"Xujing Zhang","Zhang","Queen's University"],[314,"Junhan Zhou","Zhou","Carnegie Mellon University"],[315,"Yang Zhang","Zhang","Carnegie Mellon University"],[316,"Gierad Laput","Laput","Carnegie Mellon University"],[317,"Chris Harrison","Harrison","Carnegie Mellon University"],[318,"Ken Nakagaki","Nakagaki","MIT"],[319,"Artem Dementyev","Dementyev","MIT"],[320,"Hiroshi Ishii","Ishii","Massachusetts Institute of Technology"],[321,"Mathieu Le Goc","Le Goc","Université Paris-Saclay"],[322,"Lawrence H Kim","Kim","Stanford University"],[323,"Ali Parsaei","Parsaei","Stanford University"],[324,"Jean-Daniel Fekete","Fekete","Université Paris-Saclay"],[325,"Pierre Dragicevic","Dragicevic","Université Paris-Saclay"],[326,"Hsin-Liu (Cindy) Kao","Kao","Massachusetts Institute of Technology"],[327,"Deborah Ajilo","Ajilo","MIT"],[328,"Maggie Xu","Xu","Stanford University"],[329,"Jifei Ou","Ou","MIT Media Lab"],[330,"Melina Skouras","Skouras","MIT"],[331,"Nikolaos Vlavianos","Vlavianos","Massachusetts Institute of Technology"],[332,"Felix Heibeck","Heibeck","MIT Media Lab"],[333,"Chin-Yi Cheng","Cheng","MIT"],[334,"Jannik Peters","Peters","Massachusetts Institute of Technology"],[335,"Marianela Ciolfi Felice","Ciolfi Felice","Université Paris-Sud, CNRS, Université Paris-Saclay"],[336,"Nolwenn Maudet","Maudet","Université Paris-Sud, CNRS, Université Paris-Saclay"],[337,"Michel Beaudouin-Lafon","Beaudouin-Lafon","Université Paris-Sud, CNRS, Université Paris-Saclay"],[338,"Aakar Gupta","Gupta","University of Toronto"],[339,"Muhammed Anwar","Anwar","University of Toronto"],[340,"Ravin Balakrishnan","Balakrishnan","University of Toronto"],[341,"Brian A Smith","Smith","Columbia University"],[342,"Shree K Nayar","Nayar","Columbia University"],[343,"Sang Ho Yoon","Yoon","Purdue University"],[344,"Yunbo Zhang","Zhang","Purdue University"],[345,"Ke Huo","Huo","Purdue University"],[346,"Karthik Ramani","Ramani","Purdue University"],[347,"Jaeyeon Lee","Lee","KAIST"],[348,"Geehyuk Lee","Lee","KAIST"],[349,"Youngjun Cho","Cho","University College London"],[350,"Nicolai Marquardt","Marquardt","University College London"],[351,"Nadia Bianchi-Berthouze","Bianchi-Berthouze","University College London"],[352,"Pedro Lopes","Lopes","Hasso Plattner Institute"],[353,"Doğa Yüksel","Yüksel","Hasso Plattner Institute"],[354,"François Guimbretière","Guimbretière","Cornell University"],[355,"Antony Irudayaraj","Irudayaraj","University of Toronto"],[356,"Vimal Chandran","Chandran","University of Toronto"],[357,"Goutham Palaniappan","Palaniappan","University of Toronto"],[358,"Khai N. Truong","Truong","University of Toronto"],[359,"Dennis Wolf","Wolf","Ulm University"],[360,"Eythor R Eiriksson","Eiriksson","Technical University of Denmark"],[361,"Joshua Hibschman","Hibschman","Northwestern University"],[362,"Haoqi Zhang","Zhang","Northwestern University"],[363,"Xin Rong","Rong","University of Michigan"],[364,"Shiyan Yan","Yan","University of Michigan"],[365,"Stephen Oney","Oney","University of Michigan"],[366,"Mira Dontcheva","Dontcheva","Adobe Research"],[367,"Eytan Adar","Adar","University of Michigan"],[368,"Ethan Fast","Fast","Stanford University"],[369,"Michael S Bernstein","Bernstein","Stanford University"],[370,"Sergio Orts-Escolano","Orts-Escolano","Microsoft Research"],[371,"Christoph Rhemann","Rhemann","Microsoft Research"],[372,"Sean Fanello","Fanello","Microsoft Research"],[373,"David Kim","Kim","Microsoft Research"],[374,"Adarsh Kowdle","Kowdle","Microsoft Research"],[375,"Wayne Chang","Chang","Microsoft Research"],[376,"Yury Degtyarev","Degtyarev","Microsoft Research"],[377,"Philip L Davidson","Davidson","Microsoft Research"],[378,"Sameh Khamis","Khamis","Microsoft Research"],[379,"Mingsong Dou","Dou","Microsoft Research"],[380,"Vladimir Tankovich","Tankovich","Microsoft Research"],[381,"Charles Loop","Loop","Microsoft Research"],[382,"Qin Cai","Cai","Microsoft Research"],[383,"Philip A Chou","Chou","Microsoft Research"],[384,"Sarah Mennicken","Mennicken","Microsoft Research"],[385,"Julien Valentin","Valentin","Microsoft Research"],[386,"Mathieu Nancel","Nancel","University of Waterloo"],[387,"Daniel Vogel","Vogel","University of Waterloo"],[388,"Bruno De Araujo","De Araujo","University of Toronto"],[389,"Ricardo Jota","Jota","Tactual Labs"],[390,"Géry Casiez","Casiez","Université de Lille"],[391,"Andrew M. Webb","Webb","Texas A&M University"],[392,"Michel Pahud","Pahud","Microsoft Research"],[393,"Ken Hinckley","Hinckley","Microsoft Research"],[394,"William A.S. Buxton","Buxton","Microsoft Research"],[395,"Ken Pfeuffer","Pfeuffer","Lancaster University"],[396,"Hans Gellersen","Gellersen","Lancaster University"],[397,"Xiaojun Bi","Bi","Inc."],[398,"Shumin Zhai","Zhai","Inc."],[399,"Robert Xiao","Xiao","Carnegie Mellon University"],[400,"Joanne Leong","Leong","University of Applied Sciences Upper Austria"],[401,"Patrick Parzer","Parzer","University of Applied Sciences Upper Austria"],[402,"Florian Perteneder","Perteneder","University of Applied Sciences Upper Austria"],[403,"Teo Babic","Babic","University of Applied Sciences Upper Austria"],[404,"Christian Rendl","Rendl","University of Applied Sciences Upper Austria"],[405,"Anita Vogl","Vogl","University of Applied Sciences Upper Austria"],[406,"Hubert Egger","Egger","University of Applied Sciences Upper Austria"],[407,"Alex Olwal","Olwal","Google, Inc."],[408,"Michael Haller","Haller","University of Applied Sciences Upper Austria"],[409,"Nediyana Daskalova","Daskalova","Brown University"],[410,"Danaë Metaxa-Kakavouli","Metaxa-Kakavouli","Stanford University"],[411,"Adrienne Tran","Tran","Brown University"],[412,"Nicole Nugent","Nugent","Brown University"],[413,"Julie Boergers","Boergers","Brown University"],[414,"John McGeary","McGeary","Brown University"],[415,"Jeff Huang","Huang","Brown University"],[416,"Yang Li","Li","Google Research"],[417,"Vidya Setlur","Setlur","Tableau Research"],[418,"Sarah E Battersby","Battersby","Tableau Software"],[419,"Melanie K Tory","Tory","Tableau Software"],[420,"Rich Gossweiler","Gossweiler","Tableau Software"],[421,"Angel X Chang","Chang","Tableau Software"],[422,"Brian Hempel","Hempel","University of Chicago"],[423,"Ravi Chugh","Chugh","University of Chicago"],[424,"Danielle Bragg","Bragg","University of Washington"],[425,"Shiri Azenkot","Azenkot","Cornell Tech"],[426,"Adam Kalai","Kalai","Microsoft Research"],[427,"Takeo Igarashi","Igarashi","The University of Tokyo"],[428,"Naoyuki Shono","Shono","The University of Tokyo"],[429,"Taichi Kin","Kin","The University of Tokyo"],[430,"Toki Saito","Saito","The University of Tokyo"],[431,"Hajime Kajita","Kajita","The University of Tokyo"],[432,"Naoya KOIZUMI","KOIZUMI","the University of Electro-Communications"],[433,"Takeshi Naemura","Naemura","The University of Tokyo"],[434,"David Lindlbauer","Lindlbauer","TU Berlin"],[435,"Joerg Mueller","Mueller","Aarhus University"],[436,"Marc Alexa","Alexa","TU Berlin"],[437,"Takuto Nakamura","Nakamura","University of Electro-Communications"],[438,"Asier Marzo","Marzo","University of Bristol"],[439,"Themis Omirou","Omirou","University of Bristol"],[440,"Michihiro Asakawa","Asakawa","University of Sussex"],[441,"Qiuyu Lu","Lu","Tsinghua University"],[442,"Chengpeng Mao","Mao","Tsinghua University"],[443,"Liyuan Wang","Wang","Tsinghua University"],[444,"Haipeng Mi","Mi","Tsinghua University"],[445,"Yoshihiro Watanabe","Watanabe","The University of Tokyo"],[446,"Masatoshi Ishikawa","Ishikawa","The University of Tokyo"],[447,"Scott Ruoti","Ruoti","Brigham Young University"],[448,"Jeff Andersen","Andersen","Brigham Young University"],[449,"Travis Hendershot","Hendershot","Brigham Young University"],[450,"Daniel Zappala","Zappala","Brigham Young University"],[451,"Kent Seamons","Seamons","Brigham Young University"],[452,"Sehi L'Yi","L'Yi","Seoul National University"],[453,"Jaemin Jo","Jo","Seoul National University"],[454,"Bohyoung Kim","Kim","Hankuk University of Foreign Studies"],[455,"Lea Verou","Verou","MIT"],[456,"Amy X. Zhang","Zhang","Massachusetts Institute of Technology"],[457,"David Karger","Karger","Massachusetts Institute of Technology"],[458,"Anh Truong","Truong","Adobe Systems"],[459,"Floraine Berthouzoz","Berthouzoz","Adobe Systems"],[460,"Wilmot Li","Li","Adobe Systems"],[461,"Maneesh Agrawala","Agrawala","Stanford University"],[462,"Hijung Valentina Shin","Shin","CSAIL"],[463,"Fredo Durand","Durand","CSAIL"],[464,"Amy Pavel","Pavel","Berkeley"],[465,"Dan B Goldman","Goldman","Google"],[466,"Björn Hartmann","Hartmann","University of California, Berkeley"],[467,"Alexandra Ion","Ion","Hasso Plattner Institute"],[468,"Johannes Frohnhofen","Frohnhofen","Hasso Plattner Institute"],[469,"Ludwig Wilhelm Wall","Wall","Hasso Plattner Institute"],[470,"Robert Kovacs","Kovacs","Hasso Plattner Institute"],[471,"Mirela Alistar","Alistar","Hasso Plattner Institute"],[472,"Jack I. C. Lindsay","Lindsay","Hasso Plattner Institute"],[473," Hsiang-Ting Chen","Chen","Hasso Plattner Institute"],[474,"Moran Mizrahi","Mizrahi","The Hebrew University"],[475,"Amos Golan","Golan","Massachusetts Institute of Technology"],[476,"Ariel Bezaleli  Mizrahi","Mizrahi","The Hebrew University of Jerusalem"],[477,"Rotem Gruber","Gruber","Bezalel Academy of Arts and Design"],[478,"Alexander \"Zoonder\",\"Lachnise zoonder@gmail.com","zoonder@gmail.com","Jerusalem "],[479,"Huaishu Peng","Peng","Cornell University"],[480,"James McCann","McCann","Disney Research Pittsburgh"],[481,"Kiril Vidimce","Vidimce","Massachusetts Institute of Technology"],[482,"Alexandre Kaspar","Kaspar","Massachusetts Institute of Technology"],[483,"Ye Wang","Wang","Massachusetts Institute of Technology"],[484,"Wojciech Matusik","Matusik","Massachusetts Institute of Technology"],[485,"Tomoki Shibata","Shibata","Tufts University"],[486,"Daniel Afergan","Afergan","Google, Inc."],[487,"Danielle Kong","Kong","Tufts University"],[488,"Beste F Yuksel","Yuksel","Tufts University"],[489,"Scott MacKenzie","MacKenzie","York University"],[490,"Robert J.K. Jacob","Jacob","Tufts University"],[491,"Jessalyn Alvina","Alvina","INRIA & Univ Paris-Saclay"],[492,"Joseph W Malloch","Malloch","Inria, Université Paris-Saclay"],[493,"Yi-Chi Liao","Liao","National Taiwan University"],[494,"Yi-Ling Chen","Chen","University of California"],[495,"Jo-Yu Lo","Lo","National Taiwan University"],[496,"Rong-Hao Liang","Liang","National Taiwan University"],[497,"Bing-Yu Chen","Chen","National Taiwan University"],[498,"Kenneth C Arnold","Arnold","Harvard University"],[499,"Krzysztof Z Gajos","Gajos","Harvard University"],[500,"Pao Siangliulue","Siangliulue","Harvard University"],[501,"Joel Chan","Chan","Carnegie Mellon University"],[502,"Steven P Dow","Dow","UC San Diego"],[503,"Snehalkumar (Neil) S Gaikwad","Gaikwad","Massachusetts Institute of Technology"],[504,"Durim Morina","Morina","Stanford University"],[505,"Adam Ginzberg","Ginzberg","Stanford University"],[506,"Catherine A Mullings","Mullings","Stanford University"],[507,"Shirish Goyal","Goyal","Stanford University"],[508,"Dilrukshi Gamage","Gamage","University of Moratuwa"],[509,"Mark Edmond Whiting","Whiting","Carnegie Mellon University"],[510,"Vibhor Sehgal","Sehgal","Maharaja Agrasen Institute of Technology"],[511,"Aaron Gilbee","Gilbee","Independent Researcher "],[512,"Mathias Burton","Burton","ProQuest"],[513,"Suminda Niranga Senadhipathige","Senadhipathige","Sri Lanka Institute of Information Technology"],[514,"Alipta Ballav","Ballav","Harman International "],[515,"Jasmine Lin","Lin","University of Washington"],[516,"Sharon Zhou","Zhou","Stanford University"],[517,"Rajan Vaish","Vaish","Stanford University"],[518,"Katherine Lin","Lin","Northwestern University"],[519,"Henry Spindell","Spindell","Northwestern University"],[520,"Scott Cambo","Cambo","Northwestern University"],[521,"Yongsung Kim","Kim","Northwestern University"],[522,"Anhong Guo","Guo","Carnegie Mellon University"],[523,"Haoran Qi","Qi","Carnegie Mellon University"],[524,"Samuel Christopher White","White","Carnegie Mellon University"],[525,"Suman Ghosh","Ghosh","Carnegie Mellon University"],[526,"Chieko Asakawa","Asakawa","Carnegie Mellon University"],[527,"Jeffrey Bigham","Bigham","Carnegie Mellon University"],[528,"Joanne Lo","Lo","University of California, Berkeley"],[529,"Cesar Torres","Torres","University of California, Berkeley"],[530,"Isabel Yang","Yang","UC Berkeley"],[531,"Jasper O'Leary","O'Leary","University of California, Berkeley"],[532,"Danny Kaufman","Kaufman","Adobe Research"],[533,"Eric Paulos","Paulos","University of California"],[534,"Daniel Drew","Drew","UC Berkeley"],[535,"Julie L. Newcomb","Newcomb","University of Washington"],[536,"William McGrath","McGrath","Stanford University"],[537,"Filip Maksimovic","Maksimovic","UC Berkeley"],[538,"David A Mellis","Mellis","UC Berkeley"],[539,"Chiuan Wang","Wang","National Taiwan University"],[540,"Hsuan-Ming Yeh","Yeh","National Taiwan University"],[541,"Bryan Wang","Wang","National Taiwan University"],[542,"Te-Yen Wu","Wu","National Taiwan University"],[543,"Hsin-Ruey Tsai","Tsai","National Taiwan University"],[544,"Mike Y. Chen","Chen","National Taiwan University"],[545,"Michael Wessely","Wessely","INRIA"],[546,"Theophanis Tsandilas","Tsandilas","Inria"],[547,"Shinji Sakamoto","Sakamoto","University of Tsukuba"],[548,"Keita Kanai","Kanai","University of Tsukuba"],[549,"Kazuki Takazawa","Takazawa","University of Tsukuba"],[550,"Hiraku Doi","Doi","University of Tsukuba"],[551,"Hrvoje Benko","Benko","Microsoft Research"],[552,"Christian Holz","Holz","Microsoft Research"],[553,"Mike Sinclair","Sinclair","Microsoft Research"],[554,"Eyal Ofek","Ofek","Microsoft Research"],[555,"Jingru Guo","Guo","Rhode Island School of Design"],[556,"Rodrigo Marques","Marques","MIT"],[557,"Raymond Wu","Wu","MIT"],[558,"Justin Chiu","Chiu","Massachusetts Institute of Technology"],[559,"Jun XING","XING","Autodesk Research"],[560,"Rubaiat Habib Kazi","Kazi","Autodesk Research"],[561,"Li-Yi Wei","Wei","The University of Hong Kong"],[562,"Jos Stam","Stam","Autodesk Research"],[563,"George Fitzmaurice","Fitzmaurice","Autodesk Research"],[564,"Biplab Deka","Deka","University of Illinois at Urbana-Champaign"],[565,"Zifeng Huang","Huang","University of Illinois at Urbana-Champaign"],[566,"Ranjitha Kumar","Kumar","University of Illinois at Urbana-Champaign"],[567,"Kristen Vaccaro","Vaccaro","University of Illinois"],[568,"Sunaya Shivakumar","Shivakumar","University of Illinois at Urbana Champaign"],[569,"Ziqiao Ding","Ding","University of Illinois"],[570,"Karrie Karahalios","Karahalios","University of Illinois at Urbana-Champaign"],[571,"Rorik Henrikson","Henrikson","University of Toronto"],[572,"Fanny Chevalier","Chevalier","Inria"],[573,"Karan Singh","Singh","University of Toronto"],[574,"Yongkwan Kim","Kim","KAIST (Korea Advanced Institute of Science and Technology)"],[575,"Seok-Hyung Bae","Bae","KAIST (Korea Advanced Institute of Science and Technology)"],[576,"Pei-Yu Chi","Chi","University of California, Berkeley"],[577,"Yusuke Sugano","Sugano","Max Planck Institute for Informatics"],[578,"Xucong Zhang","Zhang","Max Planck Institute for Informatics"],[579,"Andreas Bulling","Bulling","Max Planck Institute for Informatics"],[580,"Gergely Flamich","Flamich","University of St Andrews"],[581,"Patrick Schrempf","Schrempf","University of St Andrews"],[582,"David Harris-Birtill","Harris-Birtill","University of St Andrews"],[583,"Saiwen Wang","Wang","ETH Zurich "],[584,"Jie Song","Song","ETH Zurich "],[585,"Jaime Lien","Lien","Google, Inc."],[586,"Ivan Poupyrev","Poupyrev","Google, Inc."],[587,"Jun Gong","Gong","Dartmouth College"],[588,"Xing-Dong Yang","Yang","Dartmouth College"],[589,"Pourang Irani","Irani","University of Manitoba"],[590,"Eugene M. Taranta II","Taranta II","University of Central Florida"],[591,"Mehran Maghoumi","Maghoumi","University of Central Florida"],[592,"Corey R Pittman","Pittman","University of Central Florida"],[593,"Joseph J LaViola Jr.","LaViola Jr.","University of Central Florida"],[594,"Naoto Fukasawa","Fukasawa","NAOTO FUKASAWA DESIGN"]]}}